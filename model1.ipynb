{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import torch\nimport torch.utils.data\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport os\nGPU = torch.cuda.is_available()",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de2676d77993266b2611ed7614b0e4d420bbe5cd"
      },
      "cell_type": "code",
      "source": "# Grab data from csv\n\ndf = pd.read_csv('../input/aps360_airbnb1.csv', index_col=False)\ndf.head()",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "   Unnamed: 0        ...          wireless_internet\n0           0        ...                          1\n1           1        ...                          1\n2           2        ...                          1\n3           3        ...                          1\n4           4        ...                          1\n\n[5 rows x 98 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>host_is_superhost</th>\n      <th>guests_included</th>\n      <th>bathrooms</th>\n      <th>bedrooms</th>\n      <th>beds</th>\n      <th>price</th>\n      <th>cleaning_fee</th>\n      <th>instant_bookable</th>\n      <th>ballard</th>\n      <th>beacon_hill</th>\n      <th>capitol_hill</th>\n      <th>cascade</th>\n      <th>central_area</th>\n      <th>delridge</th>\n      <th>downtown</th>\n      <th>interbay</th>\n      <th>lake_city</th>\n      <th>magnolia</th>\n      <th>northgate</th>\n      <th>other_neighborhoods</th>\n      <th>queen_anne</th>\n      <th>rainier_valley</th>\n      <th>seward_park</th>\n      <th>university_district</th>\n      <th>west_seattle</th>\n      <th>Apartment</th>\n      <th>Bed &amp; Breakfast</th>\n      <th>Boat</th>\n      <th>Bungalow</th>\n      <th>Cabin</th>\n      <th>Camper/RV</th>\n      <th>Chalet</th>\n      <th>Condominium</th>\n      <th>Dorm</th>\n      <th>House</th>\n      <th>Loft</th>\n      <th>Other</th>\n      <th>Tent</th>\n      <th>Townhouse</th>\n      <th>...</th>\n      <th>air_conditioning</th>\n      <th>breakfast</th>\n      <th>buzzer/wireless_intercom</th>\n      <th>cable_tv</th>\n      <th>carbon_monoxide_detector</th>\n      <th>cat(s)</th>\n      <th>dog(s)</th>\n      <th>doorman</th>\n      <th>dryer</th>\n      <th>elevator_in_building</th>\n      <th>essentials</th>\n      <th>family/kid_friendly</th>\n      <th>fire_extinguisher</th>\n      <th>first_aid_kit</th>\n      <th>free_parking_on_premises</th>\n      <th>gym</th>\n      <th>hair_dryer</th>\n      <th>hangers</th>\n      <th>heating</th>\n      <th>hot_tub</th>\n      <th>indoor_fireplace</th>\n      <th>internet</th>\n      <th>iron</th>\n      <th>kitchen</th>\n      <th>laptop_friendly_workspace</th>\n      <th>lock_on_bedroom_door</th>\n      <th>other_pet(s)</th>\n      <th>pets_allowed</th>\n      <th>pets_live_on_this_property</th>\n      <th>pool</th>\n      <th>safety_card</th>\n      <th>shampoo</th>\n      <th>smoke_detector</th>\n      <th>smoking_allowed</th>\n      <th>suitable_for_events</th>\n      <th>tv</th>\n      <th>washer</th>\n      <th>washer_/_dryer</th>\n      <th>wheelchair_accessible</th>\n      <th>wireless_internet</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.133333</td>\n      <td>0.1250</td>\n      <td>0.142857</td>\n      <td>0.000000</td>\n      <td>0.066327</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.066667</td>\n      <td>0.1250</td>\n      <td>0.142857</td>\n      <td>0.000000</td>\n      <td>0.132653</td>\n      <td>0.133333</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0.666667</td>\n      <td>0.5625</td>\n      <td>0.714286</td>\n      <td>0.428571</td>\n      <td>0.974490</td>\n      <td>1.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0.066667</td>\n      <td>0.1250</td>\n      <td>0.000000</td>\n      <td>0.071429</td>\n      <td>0.081633</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0.400000</td>\n      <td>0.2500</td>\n      <td>0.428571</td>\n      <td>0.142857</td>\n      <td>0.438776</td>\n      <td>0.416667</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d7dca69e9dffc5c70b4207ce5ea2488e50197ebc"
      },
      "cell_type": "code",
      "source": "df_cols = list(df.columns)\ndf_cols.remove('price')\ndf_cols.remove('Unnamed: 56')\ndf_cols.remove('Unnamed: 0')\ndf_cols.append('price')\ndf = df[df_cols]\n\ninput_features = len(df_cols) - 1\nprint(input_features)\nprint(len(df))",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "95\n3793\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40dd1f91a21afe521372798eca591d5996844113"
      },
      "cell_type": "code",
      "source": "# turn data into \ndatanp = df.values.astype(np.float32)\nnp.random.seed(50) # set the numpy seed for consistent split\n\n# Data split count (train/valid/test): 2276/758/759\ntrain_set = np.random.choice(range(3793), 2276, replace=False)\ntrain_set.sort()\nvalid_set = np.random.choice([i for i in range(3793) if i not in train_set], 758, replace=False)\nvalid_set.sort()\ntest_set = [i for i in range(3793) if i not in train_set and i not in valid_set]\n\ntrain_set = datanp[train_set]\nvalid_set = datanp[valid_set]\ntest_set = datanp[test_set]\n\nseparator = torch.from_numpy(np.array(range(input_features)))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa1a79c3c9be60e265fd200ac59e4bf23e603979"
      },
      "cell_type": "code",
      "source": "class AirbnbPredictor(nn.Module):\n    '''\n    Class that holds regression model for Airbnb listing price prediction\n\n    input : feature tensor of N x _______\n    output : float value of price estimate (normalized)\n    '''\n    def __init__(self, N):\n        super(AirbnbPredictor, self).__init__()\n        self.name = 'AirbnbPredictor'\n        self.features = nn.Sequential(\n            nn.Linear(N, 10),\n            nn.ReLU(inplace=True),\n            nn.Linear(10, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return x",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def evaluate(model, loader, criterion):\n    \"\"\" Evaluate the network on the validation set.\n\n    Args:\n        model: PyTorch neural network object\n        loader: PyTorch data loader for the dataset\n        criterion: The loss function\n\n    Returns:\n        acc: A scalar for the avg classification acc over the validation set\n        loss: A scalar for the average loss function over the validation set\n    \"\"\"\n    total_loss = 0.0\n    total_epoch = 0\n\n    for i, data in enumerate(loader, 0):\n        price = torch.index_select(data, 1, torch.LongTensor([input_features]))\n        inputs = torch.index_select(data, 1, separator)\n        if GPU:\n            inputs = inputs.cuda()\n            price = price.cuda()\n\n        outputs = model(inputs)\n        total_epoch += len(price)\n        loss = criterion(outputs, price)\n        total_loss += loss.item()\n\n    loss = float(total_loss) / (i + 1)\n\n    return loss",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55164daf884556311a433cd8db9140b49f322b5e"
      },
      "cell_type": "code",
      "source": "# Training code\n\ndef train_net(model, train_set, valid_set, batch_size, learning_rate, num_epochs, name='default'):\n    ########################################################################\n    # Fixed PyTorch random seed for reproducible result\n    torch.manual_seed(1000)\n    if GPU:\n        torch.cuda.manual_seed_all(1000)\n\n    ########################################################################\n    # Setup data loaders for np arrays\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n    valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0)\n    \n    print(\"Data loaded. Starting training:\")\n\n    ########################################################################\n    # Loss function and optimizer\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n    train_loss = np.zeros(num_epochs)\n    val_acc = np.zeros(num_epochs)\n    val_loss = np.zeros(num_epochs)\n\n    ########################################################################\n    # Train the network\n    for epoch in range(num_epochs):\n        total_train_loss = 0.\n        total_epoch = 0\n\n        for i, data in enumerate(train_loader):\n            price = torch.index_select(data, 1, torch.LongTensor([input_features]))\n            inputs = torch.index_select(data, 1, separator)\n            if GPU:\n                inputs = inputs.cuda()\n                price = price.cuda()\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n\n            # Forward pass, backward pass, and optimize\n            outputs = model(inputs)\n            loss = criterion(outputs, price)\n            loss.backward()\n            optimizer.step()\n\n            # Calculate the statistics\n            total_train_loss += loss.item()\n            total_epoch += len(outputs)\n        \n        train_loss[epoch] = float(total_train_loss) / (i+1)\n        val_loss[epoch] = evaluate(model, valid_loader, criterion)\n\n        print((\"Epoch {}: Train loss: {} | \"+ \"Validation loss: {}\").format(epoch + 1, train_loss[epoch], val_loss[epoch]))\n\n        # Save the current model (checkpoint) to a file\n        model_path = \"MODEL{}_NAME{}_EPOCH{}\".format(model.name, name, epoch)\n        torch.save(model.state_dict(), model_path)\n\n    print('Finished Training')\n\n    train_loss, val_loss\n    plt.title(\"Training & Validation Loss\")\n    plt.plot(range(1,num_epochs+1), train_loss, label=\"Train\")\n    plt.plot(range(1,num_epochs+1), val_loss, label=\"Validation\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend(loc='best')\n    plt.show()",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "570a66eeff9d880035a9ce0ad9bde14daa5b61fe"
      },
      "cell_type": "code",
      "source": "test_model = AirbnbPredictor(input_features) if not GPU else AirbnbPredictor(input_features).cuda()\ntrain_net(test_model, train_set, valid_set, batch_size=32, learning_rate=3e-4, num_epochs=1000, name='default')",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data loaded. Starting training:\nEpoch 1: Train loss: 0.1476051981250445 | Validation loss: 0.14746536314487457\nEpoch 2: Train loss: 0.14435613621026278 | Validation loss: 0.14459842753907046\nEpoch 3: Train loss: 0.1418600065840615 | Validation loss: 0.14201266504824162\nEpoch 4: Train loss: 0.13884489548703036 | Validation loss: 0.1391142631570498\nEpoch 5: Train loss: 0.13624594846947324 | Validation loss: 0.13638640691836676\nEpoch 6: Train loss: 0.13362597229166162 | Validation loss: 0.13386021306117377\nEpoch 7: Train loss: 0.13160294170180956 | Validation loss: 0.13092511737098297\nEpoch 8: Train loss: 0.12814824810872474 | Validation loss: 0.1283129376048843\nEpoch 9: Train loss: 0.1262289479167925 | Validation loss: 0.1253764107823372\nEpoch 10: Train loss: 0.12353715983529885 | Validation loss: 0.12285053326437871\nEpoch 11: Train loss: 0.12085728357649511 | Validation loss: 0.1203058719014128\nEpoch 12: Train loss: 0.1183242846487297 | Validation loss: 0.11783825140446424\nEpoch 13: Train loss: 0.11601333661625783 | Validation loss: 0.11525204001615445\nEpoch 14: Train loss: 0.11310062029709418 | Validation loss: 0.11251979259153207\nEpoch 15: Train loss: 0.1102314931857917 | Validation loss: 0.11020520081122716\nEpoch 16: Train loss: 0.10853176439801852 | Validation loss: 0.10788625199347734\nEpoch 17: Train loss: 0.1062358705740836 | Validation loss: 0.10510868206620216\nEpoch 18: Train loss: 0.10341500780648655 | Validation loss: 0.1028501121327281\nEpoch 19: Train loss: 0.10103044627855222 | Validation loss: 0.10082850543161233\nEpoch 20: Train loss: 0.09922034107148647 | Validation loss: 0.09835069781790177\nEpoch 21: Train loss: 0.0967441320212351 | Validation loss: 0.09613747522234917\nEpoch 22: Train loss: 0.09468314331024885 | Validation loss: 0.09396542143076658\nEpoch 23: Train loss: 0.09236611084391673 | Validation loss: 0.09177827493598063\nEpoch 24: Train loss: 0.09056182547161977 | Validation loss: 0.08978189248591661\nEpoch 25: Train loss: 0.08772752853110433 | Validation loss: 0.08783627860248089\nEpoch 26: Train loss: 0.08653873049964507 | Validation loss: 0.08572717073063056\nEpoch 27: Train loss: 0.08423233808328708 | Validation loss: 0.08380649021516244\nEpoch 28: Train loss: 0.08207622868940234 | Validation loss: 0.08185355179011822\nEpoch 29: Train loss: 0.08026779950078991 | Validation loss: 0.08014913244793813\nEpoch 30: Train loss: 0.07843606820743945 | Validation loss: 0.07817174432178338\nEpoch 31: Train loss: 0.07671306437502305 | Validation loss: 0.07638576673343778\nEpoch 32: Train loss: 0.07537981019251877 | Validation loss: 0.07475387708594401\nEpoch 33: Train loss: 0.0738101478976508 | Validation loss: 0.07302674340705077\nEpoch 34: Train loss: 0.07167164810622732 | Validation loss: 0.07147186513369282\nEpoch 35: Train loss: 0.06996952856166495 | Validation loss: 0.0697892545722425\nEpoch 36: Train loss: 0.06906993381886019 | Validation loss: 0.06824916337306301\nEpoch 37: Train loss: 0.06698504085135129 | Validation loss: 0.06672113699217637\nEpoch 38: Train loss: 0.06563274846929643 | Validation loss: 0.06537431090449293\nEpoch 39: Train loss: 0.06414203526866105 | Validation loss: 0.06384461326524615\nEpoch 40: Train loss: 0.06256084744301108 | Validation loss: 0.06234020988146464\nEpoch 41: Train loss: 0.06145158989561929 | Validation loss: 0.061023963149636984\nEpoch 42: Train loss: 0.060078067808515496 | Validation loss: 0.0595683177622656\nEpoch 43: Train loss: 0.05846971200986041 | Validation loss: 0.05825282664348682\nEpoch 44: Train loss: 0.05700710268380741 | Validation loss: 0.057048624381422997\nEpoch 45: Train loss: 0.0561733564051489 | Validation loss: 0.05574031065528592\nEpoch 46: Train loss: 0.056305938969469733 | Validation loss: 0.05480468412861228\nEpoch 47: Train loss: 0.05402129774706231 | Validation loss: 0.05365675253172716\nEpoch 48: Train loss: 0.05278309646786915 | Validation loss: 0.05223669670522213\nEpoch 49: Train loss: 0.051313912185529865 | Validation loss: 0.05114970946063598\nEpoch 50: Train loss: 0.050539564393046826 | Validation loss: 0.050145459827035666\nEpoch 51: Train loss: 0.049190398331524596 | Validation loss: 0.04919414749989907\nEpoch 52: Train loss: 0.04837740512771739 | Validation loss: 0.048031441401690245\nEpoch 53: Train loss: 0.047310062425418034 | Validation loss: 0.04722987487912178\nEpoch 54: Train loss: 0.04639511550259259 | Validation loss: 0.046224648443361126\nEpoch 55: Train loss: 0.045395939921339355 | Validation loss: 0.04541984821359316\nEpoch 56: Train loss: 0.04487668117508292 | Validation loss: 0.044414252818872534\nEpoch 57: Train loss: 0.043652325661645994 | Validation loss: 0.04349249958371123\nEpoch 58: Train loss: 0.0429210624554091 | Validation loss: 0.04266905396555861\nEpoch 59: Train loss: 0.04205437834995488 | Validation loss: 0.04191336237515012\nEpoch 60: Train loss: 0.04133486359690627 | Validation loss: 0.04106708957503239\nEpoch 61: Train loss: 0.040475720539689064 | Validation loss: 0.040363309594492115\nEpoch 62: Train loss: 0.03980529737762279 | Validation loss: 0.03952504616851608\nEpoch 63: Train loss: 0.03914654169542094 | Validation loss: 0.03883494452262918\nEpoch 64: Train loss: 0.03820371816659139 | Validation loss: 0.03807883787279328\nEpoch 65: Train loss: 0.037827264232974916 | Validation loss: 0.037462633568793535\nEpoch 66: Train loss: 0.03674146346747875 | Validation loss: 0.03675290150567889\nEpoch 67: Train loss: 0.03615852401384877 | Validation loss: 0.036123145914946996\nEpoch 68: Train loss: 0.035603064060625106 | Validation loss: 0.035502448522796236\nEpoch 69: Train loss: 0.035052297792086996 | Validation loss: 0.03488008972878257\nEpoch 70: Train loss: 0.03450514658147262 | Validation loss: 0.03445492514098684\nEpoch 71: Train loss: 0.03386460650815732 | Validation loss: 0.03379707271233201\nEpoch 72: Train loss: 0.033414204553183585 | Validation loss: 0.033140126460542284\nEpoch 73: Train loss: 0.03249873477034271 | Validation loss: 0.03261698060669005\nEpoch 74: Train loss: 0.0322176358766026 | Validation loss: 0.03198878723196685\nEpoch 75: Train loss: 0.031534743753986225 | Validation loss: 0.03152437857352197\nEpoch 76: Train loss: 0.031337561939532556 | Validation loss: 0.031002398657922942\nEpoch 77: Train loss: 0.030590222967374656 | Validation loss: 0.030410102801397443\nEpoch 78: Train loss: 0.030180008900869224 | Validation loss: 0.030073371871064108\nEpoch 79: Train loss: 0.029877442193941936 | Validation loss: 0.029748849648361404\nEpoch 80: Train loss: 0.029347327770665288 | Validation loss: 0.029113204839328926\nEpoch 81: Train loss: 0.029198609214896958 | Validation loss: 0.028740217521165807\nEpoch 82: Train loss: 0.029946219408884645 | Validation loss: 0.02823143343751629\nEpoch 83: Train loss: 0.027969553937307663 | Validation loss: 0.027960946861033637\nEpoch 84: Train loss: 0.027775967131472297 | Validation loss: 0.027526205172762275\nEpoch 85: Train loss: 0.02744610030721459 | Validation loss: 0.027053059156363208\nEpoch 86: Train loss: 0.02671359886880964 | Validation loss: 0.026834454542646807\nEpoch 87: Train loss: 0.026565535433797374 | Validation loss: 0.026372879355524976\nEpoch 88: Train loss: 0.026448814224244818 | Validation loss: 0.026053475759302575\nEpoch 89: Train loss: 0.025785848705304995 | Validation loss: 0.02572151560646792\nEpoch 90: Train loss: 0.02528136925927053 | Validation loss: 0.025354567837590974\nEpoch 91: Train loss: 0.025182807481744222 | Validation loss: 0.024933270877227187\nEpoch 92: Train loss: 0.024897636328306463 | Validation loss: 0.024827005264038842\nEpoch 93: Train loss: 0.024648513425038092 | Validation loss: 0.02444361406378448\nEpoch 94: Train loss: 0.024139267043210566 | Validation loss: 0.02402018002855281\nEpoch 95: Train loss: 0.023879425225055054 | Validation loss: 0.023769608388344448\nEpoch 96: Train loss: 0.024427601058656972 | Validation loss: 0.02354367795245101\nEpoch 97: Train loss: 0.023497975715953443 | Validation loss: 0.023305802295605343\nEpoch 98: Train loss: 0.022966680556742683 | Validation loss: 0.022950500948354602\nEpoch 99: Train loss: 0.022786039212304685 | Validation loss: 0.022656457343449194\nEpoch 100: Train loss: 0.022513776150945988 | Validation loss: 0.022332756120401125\nEpoch 101: Train loss: 0.022453279208598867 | Validation loss: 0.02211778420799722\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 102: Train loss: 0.02226395272494604 | Validation loss: 0.021910671377554536\nEpoch 103: Train loss: 0.02214000729792234 | Validation loss: 0.021659331279806793\nEpoch 104: Train loss: 0.021657434835409124 | Validation loss: 0.02149431690728913\nEpoch 105: Train loss: 0.021774676395580173 | Validation loss: 0.021179150518340368\nEpoch 106: Train loss: 0.021392679100649223 | Validation loss: 0.020955150209677715\nEpoch 107: Train loss: 0.02088010197298394 | Validation loss: 0.020775561414969463\nEpoch 108: Train loss: 0.02082909353905254 | Validation loss: 0.020514809642918408\nEpoch 109: Train loss: 0.02055864637562384 | Validation loss: 0.02030757733155042\nEpoch 110: Train loss: 0.02055438059485621 | Validation loss: 0.02028988309515019\nEpoch 111: Train loss: 0.020017686759173457 | Validation loss: 0.01992997988903274\nEpoch 112: Train loss: 0.019902172847650945 | Validation loss: 0.01984221503759424\nEpoch 113: Train loss: 0.019689124763115413 | Validation loss: 0.019575161626562476\nEpoch 114: Train loss: 0.019610875156811543 | Validation loss: 0.019428417008991044\nEpoch 115: Train loss: 0.019534921661640208 | Validation loss: 0.019196345082794625\nEpoch 116: Train loss: 0.01921664325830837 | Validation loss: 0.019058246669980388\nEpoch 117: Train loss: 0.019140553815911215 | Validation loss: 0.0188940716907382\nEpoch 118: Train loss: 0.018941423377125628 | Validation loss: 0.018756322950745623\nEpoch 119: Train loss: 0.018748650728310976 | Validation loss: 0.018573693931102753\nEpoch 120: Train loss: 0.018715316196903586 | Validation loss: 0.018419222983842094\nEpoch 121: Train loss: 0.01836588835188498 | Validation loss: 0.018436150431322556\nEpoch 122: Train loss: 0.018325430250519678 | Validation loss: 0.018136871318953734\nEpoch 123: Train loss: 0.018231220724475052 | Validation loss: 0.017904381927413244\nEpoch 124: Train loss: 0.01805343922589802 | Validation loss: 0.01807696558535099\nEpoch 125: Train loss: 0.018693214101302955 | Validation loss: 0.017710379014412563\nEpoch 126: Train loss: 0.017703910293574963 | Validation loss: 0.01750781383210172\nEpoch 127: Train loss: 0.017649664310738444 | Validation loss: 0.01759734668303281\nEpoch 128: Train loss: 0.017406901215306587 | Validation loss: 0.017268001800403\nEpoch 129: Train loss: 0.017360301132106945 | Validation loss: 0.017178486799821258\nEpoch 130: Train loss: 0.017190566493405238 | Validation loss: 0.01701715662299345\nEpoch 131: Train loss: 0.01717411395576265 | Validation loss: 0.0169669851505508\nEpoch 132: Train loss: 0.01698051862573872 | Validation loss: 0.016820361100447673\nEpoch 133: Train loss: 0.01686226055284755 | Validation loss: 0.01671663799788803\nEpoch 134: Train loss: 0.01706914937434097 | Validation loss: 0.016542321575495105\nEpoch 135: Train loss: 0.016683875104515918 | Validation loss: 0.016454727776969474\nEpoch 136: Train loss: 0.01657233118183083 | Validation loss: 0.016444719008480508\nEpoch 137: Train loss: 0.016606910244768694 | Validation loss: 0.016195142214807372\nEpoch 138: Train loss: 0.016439985018223524 | Validation loss: 0.016105841651248436\nEpoch 139: Train loss: 0.016339527274895873 | Validation loss: 0.016090748559994\nEpoch 140: Train loss: 0.01627157107254283 | Validation loss: 0.01600332499947399\nEpoch 141: Train loss: 0.016094353515654802 | Validation loss: 0.01580968382768333\nEpoch 142: Train loss: 0.01606038969475776 | Validation loss: 0.015702125034295022\nEpoch 143: Train loss: 0.015965745409226254 | Validation loss: 0.015587821447600922\nEpoch 144: Train loss: 0.016064688220568415 | Validation loss: 0.015514640913655361\nEpoch 145: Train loss: 0.015691253304895427 | Validation loss: 0.01547367824241519\nEpoch 146: Train loss: 0.015656707303908963 | Validation loss: 0.01543059638546159\nEpoch 147: Train loss: 0.015608007612172514 | Validation loss: 0.01529892076117297\nEpoch 148: Train loss: 0.015573338307957683 | Validation loss: 0.015173173509538174\nEpoch 149: Train loss: 0.015428867268686494 | Validation loss: 0.015108893082166711\nEpoch 150: Train loss: 0.015387969466650652 | Validation loss: 0.015023296155656377\nEpoch 151: Train loss: 0.015255893515940342 | Validation loss: 0.01497112486201028\nEpoch 152: Train loss: 0.015182097468318211 | Validation loss: 0.014839625257688263\nEpoch 153: Train loss: 0.015355761395767331 | Validation loss: 0.014802526837835709\nEpoch 154: Train loss: 0.015010882932175364 | Validation loss: 0.014744212773318091\nEpoch 155: Train loss: 0.015244584627604732 | Validation loss: 0.014681517922629913\nEpoch 156: Train loss: 0.014866431812859245 | Validation loss: 0.014737233403138816\nEpoch 157: Train loss: 0.014898838151970671 | Validation loss: 0.01448349809894959\nEpoch 158: Train loss: 0.014884693056551946 | Validation loss: 0.014503256107370058\nEpoch 159: Train loss: 0.014679512752789177 | Validation loss: 0.014384351205080748\nEpoch 160: Train loss: 0.014605205001619955 | Validation loss: 0.014357700245454907\nEpoch 161: Train loss: 0.014604354729979403 | Validation loss: 0.014274202830468615\nEpoch 162: Train loss: 0.014480730906749764 | Validation loss: 0.014222770851726333\nEpoch 163: Train loss: 0.014395145932212472 | Validation loss: 0.014245155228612324\nEpoch 164: Train loss: 0.014353702808471603 | Validation loss: 0.014004268935726335\nEpoch 165: Train loss: 0.014476412241088433 | Validation loss: 0.014097573119215667\nEpoch 166: Train loss: 0.014386041270982888 | Validation loss: 0.013930874333406488\nEpoch 167: Train loss: 0.014300039439048205 | Validation loss: 0.013981584226712584\nEpoch 168: Train loss: 0.014182851856781377 | Validation loss: 0.013955766373934845\nEpoch 169: Train loss: 0.01416765795632576 | Validation loss: 0.01380891139463832\nEpoch 170: Train loss: 0.014033255516551435 | Validation loss: 0.013848105911165476\nEpoch 171: Train loss: 0.014052287048091076 | Validation loss: 0.013769730110652745\nEpoch 172: Train loss: 0.014068741441911293 | Validation loss: 0.013610234445271393\nEpoch 173: Train loss: 0.014063553153795915 | Validation loss: 0.013572898072501024\nEpoch 174: Train loss: 0.013869832205172215 | Validation loss: 0.013606684922706336\nEpoch 175: Train loss: 0.013943019769309709 | Validation loss: 0.013486480961243311\nEpoch 176: Train loss: 0.013788881465896137 | Validation loss: 0.013425862416625023\nEpoch 177: Train loss: 0.013787060031770833 | Validation loss: 0.01346187541882197\nEpoch 178: Train loss: 0.014003758027683944 | Validation loss: 0.013379167144497236\nEpoch 179: Train loss: 0.013763387616361596 | Validation loss: 0.013265726156532764\nEpoch 180: Train loss: 0.013657507409031192 | Validation loss: 0.013277929276227951\nEpoch 181: Train loss: 0.013581551857189171 | Validation loss: 0.013375471617716054\nEpoch 182: Train loss: 0.013469285182913559 | Validation loss: 0.013217833222976575\nEpoch 183: Train loss: 0.013493440262714608 | Validation loss: 0.013358918173859516\nEpoch 184: Train loss: 0.013384929256668935 | Validation loss: 0.01309219685693582\nEpoch 185: Train loss: 0.013565708197549813 | Validation loss: 0.013088093294451634\nEpoch 186: Train loss: 0.01338709502791365 | Validation loss: 0.01333948050159961\nEpoch 187: Train loss: 0.013317697603876391 | Validation loss: 0.013003357918933034\nEpoch 188: Train loss: 0.01324458599750263 | Validation loss: 0.01296384729600201\nEpoch 189: Train loss: 0.013305095548275858 | Validation loss: 0.012911621131934226\nEpoch 190: Train loss: 0.013297481229528785 | Validation loss: 0.012865151899556318\nEpoch 191: Train loss: 0.013241396550761743 | Validation loss: 0.012792341993190348\nEpoch 192: Train loss: 0.013205290849631032 | Validation loss: 0.012858292281938096\nEpoch 193: Train loss: 0.013151407895040594 | Validation loss: 0.012788315381233891\nEpoch 194: Train loss: 0.013028090722703686 | Validation loss: 0.012776363951464495\nEpoch 195: Train loss: 0.013037100259680301 | Validation loss: 0.01266252480369682\nEpoch 196: Train loss: 0.013166374352294952 | Validation loss: 0.012692258421642085\nEpoch 197: Train loss: 0.013062813060565127 | Validation loss: 0.012572657219910374\nEpoch 198: Train loss: 0.01302008080529049 | Validation loss: 0.012564827202974508\nEpoch 199: Train loss: 0.013039158133324236 | Validation loss: 0.012537807730647424\nEpoch 200: Train loss: 0.01302184607549053 | Validation loss: 0.012611846070891867\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 201: Train loss: 0.012960571123080121 | Validation loss: 0.012706446112133563\nEpoch 202: Train loss: 0.012912039936054498 | Validation loss: 0.012451335128086308\nEpoch 203: Train loss: 0.012939340713071741 | Validation loss: 0.012413781136274338\nEpoch 204: Train loss: 0.01285830547567457 | Validation loss: 0.0123900943241703\nEpoch 205: Train loss: 0.012762955519267254 | Validation loss: 0.01244835346005857\nEpoch 206: Train loss: 0.012743817118462175 | Validation loss: 0.012336235978485396\nEpoch 207: Train loss: 0.012710698149425702 | Validation loss: 0.012399984193810573\nEpoch 208: Train loss: 0.012619764249797704 | Validation loss: 0.01233724165164555\nEpoch 209: Train loss: 0.012920275653919412 | Validation loss: 0.012226896593347192\nEpoch 210: Train loss: 0.012709605565760285 | Validation loss: 0.012317976468087485\nEpoch 211: Train loss: 0.012681924039497972 | Validation loss: 0.012255945960835865\nEpoch 212: Train loss: 0.012550936662591994 | Validation loss: 0.012239722845455011\nEpoch 213: Train loss: 0.012619681268309554 | Validation loss: 0.012164890707936138\nEpoch 214: Train loss: 0.012546886204897115 | Validation loss: 0.01209984696470201\nEpoch 215: Train loss: 0.012565212537689755 | Validation loss: 0.012132443642864624\nEpoch 216: Train loss: 0.012505945096361555 | Validation loss: 0.012098913604859263\nEpoch 217: Train loss: 0.012442762672435492 | Validation loss: 0.01212752313585952\nEpoch 218: Train loss: 0.012423227405330787 | Validation loss: 0.012026936979964375\nEpoch 219: Train loss: 0.012412543790156228 | Validation loss: 0.012036536645609885\nEpoch 220: Train loss: 0.013531331231610643 | Validation loss: 0.011931039024299631\nEpoch 221: Train loss: 0.012379444191335805 | Validation loss: 0.012005338057254752\nEpoch 222: Train loss: 0.01275596589807214 | Validation loss: 0.012299913621973246\nEpoch 223: Train loss: 0.012284928460657183 | Validation loss: 0.011939444889624914\nEpoch 224: Train loss: 0.012283186751624776 | Validation loss: 0.011907887470442802\nEpoch 225: Train loss: 0.012303402298130095 | Validation loss: 0.01186098053585738\nEpoch 226: Train loss: 0.012261735669906355 | Validation loss: 0.011870327172800899\nEpoch 227: Train loss: 0.012479011848982837 | Validation loss: 0.011850587325170636\nEpoch 228: Train loss: 0.012310855756772475 | Validation loss: 0.012017380620818585\nEpoch 229: Train loss: 0.012246101997637501 | Validation loss: 0.011780031529876092\nEpoch 230: Train loss: 0.012166533480113786 | Validation loss: 0.01174811760817344\nEpoch 231: Train loss: 0.012305549045170968 | Validation loss: 0.01192572369473055\nEpoch 232: Train loss: 0.012167892166568587 | Validation loss: 0.01204233344954749\nEpoch 233: Train loss: 0.012105207718882917 | Validation loss: 0.011937825397277871\nEpoch 234: Train loss: 0.012164068990387022 | Validation loss: 0.011999572239195308\nEpoch 235: Train loss: 0.01238064766059526 | Validation loss: 0.01177298188364754\nEpoch 236: Train loss: 0.012089016679156985 | Validation loss: 0.011645424936432391\nEpoch 237: Train loss: 0.012071398966428306 | Validation loss: 0.011617416069687655\nEpoch 238: Train loss: 0.012028174474835396 | Validation loss: 0.011764487794910869\nEpoch 239: Train loss: 0.012160948768723756 | Validation loss: 0.011691331320131818\nEpoch 240: Train loss: 0.012102670191476742 | Validation loss: 0.011619320139288902\nEpoch 241: Train loss: 0.012160024547483772 | Validation loss: 0.011540262533041338\nEpoch 242: Train loss: 0.01198976107278011 | Validation loss: 0.011606136802583933\nEpoch 243: Train loss: 0.012019063966969648 | Validation loss: 0.011513503113140663\nEpoch 244: Train loss: 0.012163294537458569 | Validation loss: 0.01155043220690762\nEpoch 245: Train loss: 0.01192347419823313 | Validation loss: 0.011524952298107868\nEpoch 246: Train loss: 0.011960003363330744 | Validation loss: 0.011629712224627534\nEpoch 247: Train loss: 0.011983754100381501 | Validation loss: 0.011477334696489075\nEpoch 248: Train loss: 0.012320674451378485 | Validation loss: 0.011561402876395732\nEpoch 249: Train loss: 0.011915628319709666 | Validation loss: 0.011532852814222375\nEpoch 250: Train loss: 0.011952812136668298 | Validation loss: 0.011621398657249907\nEpoch 251: Train loss: 0.011911284068547603 | Validation loss: 0.011405399243813008\nEpoch 252: Train loss: 0.011847837776359584 | Validation loss: 0.011393152778812995\nEpoch 253: Train loss: 0.011939083795166679 | Validation loss: 0.011375879577826709\nEpoch 254: Train loss: 0.011871418314209828 | Validation loss: 0.011393190613792589\nEpoch 255: Train loss: 0.011902412567805085 | Validation loss: 0.011411106252732376\nEpoch 256: Train loss: 0.011788835645549826 | Validation loss: 0.011402881917698929\nEpoch 257: Train loss: 0.011792711577274732 | Validation loss: 0.011424263395989934\nEpoch 258: Train loss: 0.01188827030515919 | Validation loss: 0.011360754278333237\nEpoch 259: Train loss: 0.011855155093750605 | Validation loss: 0.01134558953344822\nEpoch 260: Train loss: 0.011719743413980015 | Validation loss: 0.011264524539001286\nEpoch 261: Train loss: 0.011858733403237743 | Validation loss: 0.011310699783886472\nEpoch 262: Train loss: 0.011754014561625404 | Validation loss: 0.011299832006140301\nEpoch 263: Train loss: 0.011726315417844389 | Validation loss: 0.011248177751743546\nEpoch 264: Train loss: 0.01260689195866386 | Validation loss: 0.011266346788033843\nEpoch 265: Train loss: 0.011729210317652259 | Validation loss: 0.011259970837272704\nEpoch 266: Train loss: 0.011715815008048795 | Validation loss: 0.011220142788564166\nEpoch 267: Train loss: 0.011720947572030127 | Validation loss: 0.011194928005958596\nEpoch 268: Train loss: 0.011624139437723594 | Validation loss: 0.011290373746305704\nEpoch 269: Train loss: 0.011779505675399883 | Validation loss: 0.01117201727659752\nEpoch 270: Train loss: 0.011623056003979096 | Validation loss: 0.011170133579677591\nEpoch 271: Train loss: 0.011598810928666757 | Validation loss: 0.011256420072944215\nEpoch 272: Train loss: 0.011820436675205 | Validation loss: 0.01122101010211433\nEpoch 273: Train loss: 0.011612368653812963 | Validation loss: 0.0111434780798542\nEpoch 274: Train loss: 0.011618732856327875 | Validation loss: 0.011142388956310848\nEpoch 275: Train loss: 0.01160228092663197 | Validation loss: 0.011136699739533166\nEpoch 276: Train loss: 0.011614658397674147 | Validation loss: 0.011172472305285433\nEpoch 277: Train loss: 0.011632693187695824 | Validation loss: 0.011133652898327758\nEpoch 278: Train loss: 0.011546156838044731 | Validation loss: 0.011126415066731473\nEpoch 279: Train loss: 0.011507949073954174 | Validation loss: 0.011239330963386843\nEpoch 280: Train loss: 0.011526757385581732 | Validation loss: 0.011078608765577277\nEpoch 281: Train loss: 0.011492281237022124 | Validation loss: 0.01121443931090956\nEpoch 282: Train loss: 0.011531268854418563 | Validation loss: 0.011029186503340801\nEpoch 283: Train loss: 0.012212850483289609 | Validation loss: 0.01106914693567281\nEpoch 284: Train loss: 0.011516509409476485 | Validation loss: 0.011031313450075686\nEpoch 285: Train loss: 0.011673312103893194 | Validation loss: 0.011194586559819678\nEpoch 286: Train loss: 0.011537649089910297 | Validation loss: 0.011010778175356487\nEpoch 287: Train loss: 0.011478470133927962 | Validation loss: 0.011011149365610132\nEpoch 288: Train loss: 0.01146626723736214 | Validation loss: 0.011009776460317275\nEpoch 289: Train loss: 0.011454049061285332 | Validation loss: 0.010962021381904682\nEpoch 290: Train loss: 0.011581898835073743 | Validation loss: 0.011202138382941484\nEpoch 291: Train loss: 0.011414763212087564 | Validation loss: 0.010961476558198532\nEpoch 292: Train loss: 0.011413867236115038 | Validation loss: 0.010978354451557001\nEpoch 293: Train loss: 0.011601205359006094 | Validation loss: 0.010960619469794134\nEpoch 294: Train loss: 0.01143767851560066 | Validation loss: 0.011022170210101953\nEpoch 295: Train loss: 0.01163665679956062 | Validation loss: 0.010923370777163655\nEpoch 296: Train loss: 0.011448157149263553 | Validation loss: 0.010919741374285271\nEpoch 297: Train loss: 0.011377500874611238 | Validation loss: 0.011108956474345177\nEpoch 298: Train loss: 0.011435774400726788 | Validation loss: 0.011002269301873943\nEpoch 299: Train loss: 0.011383569864038792 | Validation loss: 0.011207862582523376\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 300: Train loss: 0.011654238404136978 | Validation loss: 0.010874458491646996\nEpoch 301: Train loss: 0.011399934158867432 | Validation loss: 0.010883939103223383\nEpoch 302: Train loss: 0.011366187743583901 | Validation loss: 0.010842979691612223\nEpoch 303: Train loss: 0.011727880783534298 | Validation loss: 0.01087866370410969\nEpoch 304: Train loss: 0.011423090576297708 | Validation loss: 0.01089327516577517\nEpoch 305: Train loss: 0.01130991529013651 | Validation loss: 0.010903599963057786\nEpoch 306: Train loss: 0.011475501633766625 | Validation loss: 0.010831811639945954\nEpoch 307: Train loss: 0.01132834058888774 | Validation loss: 0.010949082594985763\nEpoch 308: Train loss: 0.011379193870298978 | Validation loss: 0.010923769402628144\nEpoch 309: Train loss: 0.01139364275150001 | Validation loss: 0.01086387992836535\nEpoch 310: Train loss: 0.011312945642405085 | Validation loss: 0.010804950667079538\nEpoch 311: Train loss: 0.011373413176948411 | Validation loss: 0.010842439252883196\nEpoch 312: Train loss: 0.011522278105581386 | Validation loss: 0.010947696786994735\nEpoch 313: Train loss: 0.011401105458692959 | Validation loss: 0.010767863694733629\nEpoch 314: Train loss: 0.011303627061554126 | Validation loss: 0.010917759073587755\nEpoch 315: Train loss: 0.011540478127749844 | Validation loss: 0.010751164828737577\nEpoch 316: Train loss: 0.011313758893973298 | Validation loss: 0.010745882464107126\nEpoch 317: Train loss: 0.011531703232321888 | Validation loss: 0.01075798129507651\nEpoch 318: Train loss: 0.013712351061662452 | Validation loss: 0.010794291874238601\nEpoch 319: Train loss: 0.01127965221207382 | Validation loss: 0.010880699284219494\nEpoch 320: Train loss: 0.011287350369255163 | Validation loss: 0.011085838079452515\nEpoch 321: Train loss: 0.012645765666901652 | Validation loss: 0.010761832226611054\nEpoch 322: Train loss: 0.011339646595944133 | Validation loss: 0.01099644541197146\nEpoch 323: Train loss: 0.011237488160582466 | Validation loss: 0.010762660317899039\nEpoch 324: Train loss: 0.011278279009275138 | Validation loss: 0.010702839567481229\nEpoch 325: Train loss: 0.011263410683669563 | Validation loss: 0.010803327953908592\nEpoch 326: Train loss: 0.01136499574770116 | Validation loss: 0.010742522698516646\nEpoch 327: Train loss: 0.011193254225266477 | Validation loss: 0.010711554476680854\nEpoch 328: Train loss: 0.011411459877207462 | Validation loss: 0.01070580924473082\nEpoch 329: Train loss: 0.011354244947950873 | Validation loss: 0.010726255985597769\nEpoch 330: Train loss: 0.011168157992263636 | Validation loss: 0.01069932448444888\nEpoch 331: Train loss: 0.011191893656763973 | Validation loss: 0.010775725415442139\nEpoch 332: Train loss: 0.011157719331094995 | Validation loss: 0.010808797572584202\nEpoch 333: Train loss: 0.011226388341229823 | Validation loss: 0.010764781385660172\nEpoch 334: Train loss: 0.011144743352714512 | Validation loss: 0.010654733167029917\nEpoch 335: Train loss: 0.011140509126966612 | Validation loss: 0.010634624806698412\nEpoch 336: Train loss: 0.011237893977926837 | Validation loss: 0.0106170725193806\nEpoch 337: Train loss: 0.011194093405113867 | Validation loss: 0.010787528823129833\nEpoch 338: Train loss: 0.011154686010235714 | Validation loss: 0.010611231380607933\nEpoch 339: Train loss: 0.011245280316668667 | Validation loss: 0.010635605877420554\nEpoch 340: Train loss: 0.011127886828035116 | Validation loss: 0.010664441991442194\nEpoch 341: Train loss: 0.01140002480113051 | Validation loss: 0.01064177971178045\nEpoch 342: Train loss: 0.0111202267710016 | Validation loss: 0.010682110868704816\nEpoch 343: Train loss: 0.011131869522311414 | Validation loss: 0.010682146064937115\nEpoch 344: Train loss: 0.011116321062824378 | Validation loss: 0.010683330881875008\nEpoch 345: Train loss: 0.011129023862950917 | Validation loss: 0.010639266460202634\nEpoch 346: Train loss: 0.01127278962586489 | Validation loss: 0.010635733682041367\nEpoch 347: Train loss: 0.011112100292747427 | Validation loss: 0.010562776762526482\nEpoch 348: Train loss: 0.011164348430207206 | Validation loss: 0.010595524586581936\nEpoch 349: Train loss: 0.01107315978394278 | Validation loss: 0.010662873197967807\nEpoch 350: Train loss: 0.011148252595578216 | Validation loss: 0.010822935514928153\nEpoch 351: Train loss: 0.011184292522052096 | Validation loss: 0.010632508822406331\nEpoch 352: Train loss: 0.011122686542674072 | Validation loss: 0.010597954872840395\nEpoch 353: Train loss: 0.011175001182386445 | Validation loss: 0.01067374664125964\nEpoch 354: Train loss: 0.011224736320501607 | Validation loss: 0.010604896378936246\nEpoch 355: Train loss: 0.011094691852728525 | Validation loss: 0.010571229582031568\nEpoch 356: Train loss: 0.011109245779354952 | Validation loss: 0.010622798911451051\nEpoch 357: Train loss: 0.01110910669538296 | Validation loss: 0.010797387551671514\nEpoch 358: Train loss: 0.011052799637481157 | Validation loss: 0.01055518101202324\nEpoch 359: Train loss: 0.011079838434751663 | Validation loss: 0.010580188400732974\nEpoch 360: Train loss: 0.011057122723690959 | Validation loss: 0.010538420407101512\nEpoch 361: Train loss: 0.011116011064991148 | Validation loss: 0.010582646063994616\nEpoch 362: Train loss: 0.011039617949993245 | Validation loss: 0.01079382774575303\nEpoch 363: Train loss: 0.01105971241162883 | Validation loss: 0.010516959805196771\nEpoch 364: Train loss: 0.011330975581788354 | Validation loss: 0.010516664440122744\nEpoch 365: Train loss: 0.01103735743284536 | Validation loss: 0.010528676328249276\nEpoch 366: Train loss: 0.01106003118911758 | Validation loss: 0.010672251208840558\nEpoch 367: Train loss: 0.011058766103815287 | Validation loss: 0.01049336172097052\nEpoch 368: Train loss: 0.011080481807261499 | Validation loss: 0.010540409692718336\nEpoch 369: Train loss: 0.011066221431570334 | Validation loss: 0.010666955611668527\nEpoch 370: Train loss: 0.011648930576888638 | Validation loss: 0.010566095937974751\nEpoch 371: Train loss: 0.011024213014429228 | Validation loss: 0.010507511382456869\nEpoch 372: Train loss: 0.011355990123572864 | Validation loss: 0.010530551856694123\nEpoch 373: Train loss: 0.012341026873198442 | Validation loss: 0.010560470594403645\nEpoch 374: Train loss: 0.011023312955836041 | Validation loss: 0.010477663289445141\nEpoch 375: Train loss: 0.011003595395272391 | Validation loss: 0.010496395038596043\nEpoch 376: Train loss: 0.011063792907710498 | Validation loss: 0.010476981716540953\nEpoch 377: Train loss: 0.010981517499506785 | Validation loss: 0.01049035539229711\nEpoch 378: Train loss: 0.010993680645090839 | Validation loss: 0.010521294840145856\nEpoch 379: Train loss: 0.011014426165881256 | Validation loss: 0.010439239461751034\nEpoch 380: Train loss: 0.011173746855800144 | Validation loss: 0.010482933624492338\nEpoch 381: Train loss: 0.010989204094383039 | Validation loss: 0.010442126484122127\nEpoch 382: Train loss: 0.010996690505029013 | Validation loss: 0.010468126507475972\nEpoch 383: Train loss: 0.010942006629193202 | Validation loss: 0.010497280532338968\nEpoch 384: Train loss: 0.010955901195605596 | Validation loss: 0.01058405451476574\nEpoch 385: Train loss: 0.010997638940655937 | Validation loss: 0.010462429161028316\nEpoch 386: Train loss: 0.011013768768558899 | Validation loss: 0.010453542248190692\nEpoch 387: Train loss: 0.010965975990984589 | Validation loss: 0.010476159145279476\nEpoch 388: Train loss: 0.010968701335109977 | Validation loss: 0.010491441605457416\nEpoch 389: Train loss: 0.010960020913949443 | Validation loss: 0.01048182393424213\nEpoch 390: Train loss: 0.01116189032068683 | Validation loss: 0.010441238157606373\nEpoch 391: Train loss: 0.01096905069749078 | Validation loss: 0.01040252938400954\nEpoch 392: Train loss: 0.010952797676307253 | Validation loss: 0.010414929362013936\nEpoch 393: Train loss: 0.011773523098478714 | Validation loss: 0.010452237368250886\nEpoch 394: Train loss: 0.011004982121650957 | Validation loss: 0.010519702860619873\nEpoch 395: Train loss: 0.010947992080926068 | Validation loss: 0.010462475242093205\nEpoch 396: Train loss: 0.010903934853963114 | Validation loss: 0.010420736061253896\nEpoch 397: Train loss: 0.011016925661048541 | Validation loss: 0.010490284078211213\nEpoch 398: Train loss: 0.010925207157722779 | Validation loss: 0.010460798803251237\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 399: Train loss: 0.010983051887402931 | Validation loss: 0.010450979384283224\nEpoch 400: Train loss: 0.010917288024858054 | Validation loss: 0.010407269738304118\nEpoch 401: Train loss: 0.010928850908789577 | Validation loss: 0.010449400870129466\nEpoch 402: Train loss: 0.010946608876110986 | Validation loss: 0.010477706848178059\nEpoch 403: Train loss: 0.010922935445301441 | Validation loss: 0.010376180541546395\nEpoch 404: Train loss: 0.010968350252369419 | Validation loss: 0.010410196671728045\nEpoch 405: Train loss: 0.010910609368390093 | Validation loss: 0.010378393480399003\nEpoch 406: Train loss: 0.01092776201484311 | Validation loss: 0.010396656483256569\nEpoch 407: Train loss: 0.01091403934212091 | Validation loss: 0.010402441315818578\nEpoch 408: Train loss: 0.0109701174725261 | Validation loss: 0.010394390788860619\nEpoch 409: Train loss: 0.011216535485194376 | Validation loss: 0.01045945956138894\nEpoch 410: Train loss: 0.011070002261678584 | Validation loss: 0.010349504804859558\nEpoch 411: Train loss: 0.010885030373982672 | Validation loss: 0.010377991265462091\nEpoch 412: Train loss: 0.010916100814938545 | Validation loss: 0.01039560705733796\nEpoch 413: Train loss: 0.010918185085756704 | Validation loss: 0.010334661385665337\nEpoch 414: Train loss: 0.010888152561771372 | Validation loss: 0.010398088352909932\nEpoch 415: Train loss: 0.010900014752729071 | Validation loss: 0.01036253934338068\nEpoch 416: Train loss: 0.010882666767833548 | Validation loss: 0.010344835366898527\nEpoch 417: Train loss: 0.01088795623056487 | Validation loss: 0.010404230677522719\nEpoch 418: Train loss: 0.010951644039273055 | Validation loss: 0.010328928280311326\nEpoch 419: Train loss: 0.010921669273456145 | Validation loss: 0.010291390082178017\nEpoch 420: Train loss: 0.010895503199814508 | Validation loss: 0.01038755823780472\nEpoch 421: Train loss: 0.011042470788298588 | Validation loss: 0.010306147577163452\nEpoch 422: Train loss: 0.010888201650232077 | Validation loss: 0.010464183190682283\nEpoch 423: Train loss: 0.01091223223031395 | Validation loss: 0.010305680800229311\nEpoch 424: Train loss: 0.010850337107614096 | Validation loss: 0.010329604459305605\nEpoch 425: Train loss: 0.01116893591824919 | Validation loss: 0.010450423675744483\nEpoch 426: Train loss: 0.01084470660943124 | Validation loss: 0.010422512229221562\nEpoch 427: Train loss: 0.010905417836814498 | Validation loss: 0.010297392417366305\nEpoch 428: Train loss: 0.010857804626640346 | Validation loss: 0.010435711379007747\nEpoch 429: Train loss: 0.010860752776756676 | Validation loss: 0.010295294069995483\nEpoch 430: Train loss: 0.010903280635830015 | Validation loss: 0.010349762024513135\nEpoch 431: Train loss: 0.01084257327602245 | Validation loss: 0.01028472864224265\nEpoch 432: Train loss: 0.010841669217269454 | Validation loss: 0.010301099643887332\nEpoch 433: Train loss: 0.010886321911433091 | Validation loss: 0.010395343648269773\nEpoch 434: Train loss: 0.010926861587601403 | Validation loss: 0.01047595990045617\nEpoch 435: Train loss: 0.010906278181614147 | Validation loss: 0.010327866902419677\nEpoch 436: Train loss: 0.010825158471965956 | Validation loss: 0.010270333887698749\nEpoch 437: Train loss: 0.01082594551715172 | Validation loss: 0.010304772935342044\nEpoch 438: Train loss: 0.010994415727004202 | Validation loss: 0.010301138371384392\nEpoch 439: Train loss: 0.010824963592717217 | Validation loss: 0.010314630228094757\nEpoch 440: Train loss: 0.010821479812471403 | Validation loss: 0.010343910767308747\nEpoch 441: Train loss: 0.01085343568249502 | Validation loss: 0.010282637609634548\nEpoch 442: Train loss: 0.010847240721987974 | Validation loss: 0.010274188864665726\nEpoch 443: Train loss: 0.010841605305257771 | Validation loss: 0.010329686648522815\nEpoch 444: Train loss: 0.01092595233866531 | Validation loss: 0.01029401815806826\nEpoch 445: Train loss: 0.010818429698701948 | Validation loss: 0.010441480941760043\nEpoch 446: Train loss: 0.010803343505055333 | Validation loss: 0.010267809372938549\nEpoch 447: Train loss: 0.011127295945900388 | Validation loss: 0.01039538064893956\nEpoch 448: Train loss: 0.010938073783310957 | Validation loss: 0.010256190415627012\nEpoch 449: Train loss: 0.010921784068664743 | Validation loss: 0.010466534236911684\nEpoch 450: Train loss: 0.010893970165246477 | Validation loss: 0.010239035220971951\nEpoch 451: Train loss: 0.010824090102687478 | Validation loss: 0.010380814666859806\nEpoch 452: Train loss: 0.01081547516837923 | Validation loss: 0.010248789185425267\nEpoch 453: Train loss: 0.010783835880122043 | Validation loss: 0.010223665604523072\nEpoch 454: Train loss: 0.01080161852425792 | Validation loss: 0.010313442190332959\nEpoch 455: Train loss: 0.010808545307049321 | Validation loss: 0.01022217283025384\nEpoch 456: Train loss: 0.010800576810854383 | Validation loss: 0.01056944188894704\nEpoch 457: Train loss: 0.01085974164824519 | Validation loss: 0.010303960589226335\nEpoch 458: Train loss: 0.010776959500314357 | Validation loss: 0.010292551383220902\nEpoch 459: Train loss: 0.010778348737706741 | Validation loss: 0.010283007752150297\nEpoch 460: Train loss: 0.010925899938835451 | Validation loss: 0.010319751435114691\nEpoch 461: Train loss: 0.010944489282943929 | Validation loss: 0.010232223808998242\nEpoch 462: Train loss: 0.010922237926731922 | Validation loss: 0.010242711131771406\nEpoch 463: Train loss: 0.010781952225240983 | Validation loss: 0.01020664536432984\nEpoch 464: Train loss: 0.010779117738517622 | Validation loss: 0.010221745042751232\nEpoch 465: Train loss: 0.010807361628394574 | Validation loss: 0.010218752974954745\nEpoch 466: Train loss: 0.010969670748131143 | Validation loss: 0.010278992005623877\nEpoch 467: Train loss: 0.010775547542531664 | Validation loss: 0.010293197527062148\nEpoch 468: Train loss: 0.011017058426255567 | Validation loss: 0.010201562972118458\nEpoch 469: Train loss: 0.010804543158479242 | Validation loss: 0.010167247344118854\nEpoch 470: Train loss: 0.010802492253585823 | Validation loss: 0.01021159440279007\nEpoch 471: Train loss: 0.010817062927849798 | Validation loss: 0.01024386853290101\nEpoch 472: Train loss: 0.0107556048347356 | Validation loss: 0.01023054268443957\nEpoch 473: Train loss: 0.01075394938298915 | Validation loss: 0.0103047175022463\nEpoch 474: Train loss: 0.01077704613756699 | Validation loss: 0.01023325688826541\nEpoch 475: Train loss: 0.010823353743439333 | Validation loss: 0.010274898610077798\nEpoch 476: Train loss: 0.010759597181782333 | Validation loss: 0.010306689626304433\nEpoch 477: Train loss: 0.01082098162280292 | Validation loss: 0.01016036692696313\nEpoch 478: Train loss: 0.011003176177230975 | Validation loss: 0.010160823847400025\nEpoch 479: Train loss: 0.010738863425406938 | Validation loss: 0.01025933283381164\nEpoch 480: Train loss: 0.010774731354710335 | Validation loss: 0.010168264717018852\nEpoch 481: Train loss: 0.010776691539730463 | Validation loss: 0.010194096559037765\nEpoch 482: Train loss: 0.010777639836305752 | Validation loss: 0.010146536699418599\nEpoch 483: Train loss: 0.010801792280593267 | Validation loss: 0.010333873641987642\nEpoch 484: Train loss: 0.01075947599666607 | Validation loss: 0.010357976580659548\nEpoch 485: Train loss: 0.010733982768013246 | Validation loss: 0.010314169902509699\nEpoch 486: Train loss: 0.010718359480961226 | Validation loss: 0.010322971424708763\nEpoch 487: Train loss: 0.010805260884808376 | Validation loss: 0.010221270999560753\nEpoch 488: Train loss: 0.010738889544477893 | Validation loss: 0.010251353359005103\nEpoch 489: Train loss: 0.010720894025224779 | Validation loss: 0.010205371868020544\nEpoch 490: Train loss: 0.010847382250681726 | Validation loss: 0.010261487836639086\nEpoch 491: Train loss: 0.010752296062289841 | Validation loss: 0.010416179352129499\nEpoch 492: Train loss: 0.010734777260545848 | Validation loss: 0.010323397989850491\nEpoch 493: Train loss: 0.010734019910968427 | Validation loss: 0.010195011893908182\nEpoch 494: Train loss: 0.010730047616460878 | Validation loss: 0.010221644576328496\nEpoch 495: Train loss: 0.01081941339200259 | Validation loss: 0.01017009800610443\nEpoch 496: Train loss: 0.010749627008206315 | Validation loss: 0.01027004337326313\nEpoch 497: Train loss: 0.010890940178392662 | Validation loss: 0.010200153337791562\nEpoch 498: Train loss: 0.010920306596542813 | Validation loss: 0.010196718484318504\nEpoch 499: Train loss: 0.010790191015176889 | Validation loss: 0.010111872067985436\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 500: Train loss: 0.010999217302823026 | Validation loss: 0.010243952293725064\nEpoch 501: Train loss: 0.010687758335229268 | Validation loss: 0.010196380026172847\nEpoch 502: Train loss: 0.010701614008414455 | Validation loss: 0.010143887212810418\nEpoch 503: Train loss: 0.010696822599533739 | Validation loss: 0.010142810322577134\nEpoch 504: Train loss: 0.010716095896592984 | Validation loss: 0.010110241467676436\nEpoch 505: Train loss: 0.010693122872099694 | Validation loss: 0.010140994389075786\nEpoch 506: Train loss: 0.010760100642477887 | Validation loss: 0.010096319757091502\nEpoch 507: Train loss: 0.010744834959041327 | Validation loss: 0.010399248140553633\nEpoch 508: Train loss: 0.010708679584139544 | Validation loss: 0.010132400260772556\nEpoch 509: Train loss: 0.010678877908503637 | Validation loss: 0.010136080449835086\nEpoch 510: Train loss: 0.011182120053692617 | Validation loss: 0.010095301375258714\nEpoch 511: Train loss: 0.01086667461721744 | Validation loss: 0.010187371187688163\nEpoch 512: Train loss: 0.010952996080353236 | Validation loss: 0.010144938384958854\nEpoch 513: Train loss: 0.01079154467313654 | Validation loss: 0.010108498254946122\nEpoch 514: Train loss: 0.010703131921925686 | Validation loss: 0.010230193933239207\nEpoch 515: Train loss: 0.010679632354165532 | Validation loss: 0.010169900556017334\nEpoch 516: Train loss: 0.010673920478438959 | Validation loss: 0.010325191736531755\nEpoch 517: Train loss: 0.011157919633357475 | Validation loss: 0.010086153760009134\nEpoch 518: Train loss: 0.010719771215614552 | Validation loss: 0.010189170755135516\nEpoch 519: Train loss: 0.010670055528559411 | Validation loss: 0.010135061146381\nEpoch 520: Train loss: 0.010652286724911796 | Validation loss: 0.010079846261457229\nEpoch 521: Train loss: 0.01079315291200247 | Validation loss: 0.01016282148581619\nEpoch 522: Train loss: 0.01072776264562789 | Validation loss: 0.010101809331293529\nEpoch 523: Train loss: 0.010666216403478757 | Validation loss: 0.010098517329121629\nEpoch 524: Train loss: 0.010736162547901686 | Validation loss: 0.010179924400290474\nEpoch 525: Train loss: 0.010668641884371432 | Validation loss: 0.010316486984568959\nEpoch 526: Train loss: 0.010692299489164725 | Validation loss: 0.010340147923367718\nEpoch 527: Train loss: 0.010671588184777647 | Validation loss: 0.010238437117853513\nEpoch 528: Train loss: 0.010744182884486185 | Validation loss: 0.01020853923788915\nEpoch 529: Train loss: 0.010643888120992213 | Validation loss: 0.0101732745145758\nEpoch 530: Train loss: 0.010724352346086461 | Validation loss: 0.010087505826959386\nEpoch 531: Train loss: 0.01074912952673104 | Validation loss: 0.010194928715160737\nEpoch 532: Train loss: 0.010864981188206002 | Validation loss: 0.010164958948735148\nEpoch 533: Train loss: 0.010675147201658951 | Validation loss: 0.010131199475533018\nEpoch 534: Train loss: 0.010897030555901842 | Validation loss: 0.010098285740241408\nEpoch 535: Train loss: 0.010669639424627854 | Validation loss: 0.010157057646817217\nEpoch 536: Train loss: 0.010837752456104176 | Validation loss: 0.010263797332299873\nEpoch 537: Train loss: 0.010690610057079337 | Validation loss: 0.010086712757280717\nEpoch 538: Train loss: 0.010688380810380396 | Validation loss: 0.010103297187015414\nEpoch 539: Train loss: 0.010765397352062993 | Validation loss: 0.010069915500935167\nEpoch 540: Train loss: 0.010708348689756045 | Validation loss: 0.010056469686484585\nEpoch 541: Train loss: 0.010625906006478343 | Validation loss: 0.010366744420025498\nEpoch 542: Train loss: 0.010662162695856145 | Validation loss: 0.010048431480148187\nEpoch 543: Train loss: 0.010624789246422652 | Validation loss: 0.010134108092946311\nEpoch 544: Train loss: 0.010678927472326905 | Validation loss: 0.010130399197805673\nEpoch 545: Train loss: 0.010942964303669415 | Validation loss: 0.010253141469244534\nEpoch 546: Train loss: 0.010843997691861458 | Validation loss: 0.010066181033228835\nEpoch 547: Train loss: 0.010623675974784419 | Validation loss: 0.010191231704084203\nEpoch 548: Train loss: 0.010652771177572302 | Validation loss: 0.010134646649627635\nEpoch 549: Train loss: 0.010638326603738178 | Validation loss: 0.010122286737896502\nEpoch 550: Train loss: 0.010875120613491163 | Validation loss: 0.010091252974234521\nEpoch 551: Train loss: 0.01062980895060011 | Validation loss: 0.010045465353565911\nEpoch 552: Train loss: 0.010664802552330203 | Validation loss: 0.010124134190846235\nEpoch 553: Train loss: 0.010639574291417375 | Validation loss: 0.010131486070652803\nEpoch 554: Train loss: 0.010677018222243836 | Validation loss: 0.010089047233729312\nEpoch 555: Train loss: 0.010853549003109543 | Validation loss: 0.010028287302702665\nEpoch 556: Train loss: 0.01072575992374267 | Validation loss: 0.010026473920637121\nEpoch 557: Train loss: 0.010644332712723149 | Validation loss: 0.010118723129077504\nEpoch 558: Train loss: 0.010628481964684196 | Validation loss: 0.010189285579447946\nEpoch 559: Train loss: 0.01072152951059656 | Validation loss: 0.010065457211264098\nEpoch 560: Train loss: 0.010673297750246193 | Validation loss: 0.010163430367053175\nEpoch 561: Train loss: 0.010689443856891658 | Validation loss: 0.01011325690584878\nEpoch 562: Train loss: 0.010655408734843755 | Validation loss: 0.010055436558711031\nEpoch 563: Train loss: 0.01065791951582974 | Validation loss: 0.010058298512982825\nEpoch 564: Train loss: 0.010642579494742677 | Validation loss: 0.010088242794154212\nEpoch 565: Train loss: 0.01064434633978332 | Validation loss: 0.01003554870840162\nEpoch 566: Train loss: 0.010794410401205014 | Validation loss: 0.010251206287648529\nEpoch 567: Train loss: 0.010645318223396316 | Validation loss: 0.010111613102102032\nEpoch 568: Train loss: 0.010621846647053543 | Validation loss: 0.010125617651889721\nEpoch 569: Train loss: 0.010585747652738873 | Validation loss: 0.010116027493495494\nEpoch 570: Train loss: 0.010704199017103141 | Validation loss: 0.010060614285369715\nEpoch 571: Train loss: 0.010604076669551432 | Validation loss: 0.010019021632615477\nEpoch 572: Train loss: 0.010700855730748218 | Validation loss: 0.010240454052109271\nEpoch 573: Train loss: 0.010597009538388293 | Validation loss: 0.01002164144301787\nEpoch 574: Train loss: 0.010662307390399898 | Validation loss: 0.009995905614535635\nEpoch 575: Train loss: 0.010688453650800511 | Validation loss: 0.010077279701363295\nEpoch 576: Train loss: 0.010765817908880612 | Validation loss: 0.010039970424259081\nEpoch 577: Train loss: 0.010633729142783623 | Validation loss: 0.010090856560661146\nEpoch 578: Train loss: 0.010618801849583784 | Validation loss: 0.009993682518446198\nEpoch 579: Train loss: 0.010596494656056166 | Validation loss: 0.010011426726123318\nEpoch 580: Train loss: 0.010580061626064384 | Validation loss: 0.010036328176890189\nEpoch 581: Train loss: 0.010686964760275764 | Validation loss: 0.01004387546951572\nEpoch 582: Train loss: 0.010634516663331952 | Validation loss: 0.009999696650387099\nEpoch 583: Train loss: 0.01058476959911382 | Validation loss: 0.010039385388760516\nEpoch 584: Train loss: 0.010889915742963139 | Validation loss: 0.010019336400243143\nEpoch 585: Train loss: 0.010579474697199961 | Validation loss: 0.010081125287494311\nEpoch 586: Train loss: 0.010577566556296207 | Validation loss: 0.009998535708291456\nEpoch 587: Train loss: 0.010644536656198196 | Validation loss: 0.010095574068448817\nEpoch 588: Train loss: 0.010780294322305255 | Validation loss: 0.010031479177996516\nEpoch 589: Train loss: 0.01083564335325112 | Validation loss: 0.0100561049572813\nEpoch 590: Train loss: 0.010595327531013431 | Validation loss: 0.009958249487681314\nEpoch 591: Train loss: 0.010603269895202376 | Validation loss: 0.009984313481254503\nEpoch 592: Train loss: 0.010728017663091628 | Validation loss: 0.009965892599817986\nEpoch 593: Train loss: 0.010565307731869526 | Validation loss: 0.010075303667690605\nEpoch 594: Train loss: 0.010578896856815036 | Validation loss: 0.009961991910434639\nEpoch 595: Train loss: 0.010603031111208515 | Validation loss: 0.010009509957550714\nEpoch 596: Train loss: 0.010608671459421102 | Validation loss: 0.009980910186034938\nEpoch 597: Train loss: 0.010566356019504989 | Validation loss: 0.010035529752106717\nEpoch 598: Train loss: 0.010570134188229632 | Validation loss: 0.009983962857707715\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 599: Train loss: 0.010584407288762223 | Validation loss: 0.010154229608209183\nEpoch 600: Train loss: 0.010559081599543182 | Validation loss: 0.01000789160025306\nEpoch 601: Train loss: 0.010603974469833903 | Validation loss: 0.009984966115249941\nEpoch 602: Train loss: 0.010586323633712405 | Validation loss: 0.010036371784129491\nEpoch 603: Train loss: 0.010709331729837382 | Validation loss: 0.010040098471411815\nEpoch 604: Train loss: 0.010551241342909634 | Validation loss: 0.01002234853998137\nEpoch 605: Train loss: 0.010550376509652577 | Validation loss: 0.010075914285456141\nEpoch 606: Train loss: 0.010588315134454105 | Validation loss: 0.010102182743139565\nEpoch 607: Train loss: 0.010546349910631156 | Validation loss: 0.00996001282085975\nEpoch 608: Train loss: 0.010663640588366738 | Validation loss: 0.009972963947802782\nEpoch 609: Train loss: 0.010566842512667386 | Validation loss: 0.009961126566243669\nEpoch 610: Train loss: 0.010684534907341003 | Validation loss: 0.010063112926824639\nEpoch 611: Train loss: 0.010715673292805959 | Validation loss: 0.009972470443851003\nEpoch 612: Train loss: 0.010536954859465672 | Validation loss: 0.010349541126439968\nEpoch 613: Train loss: 0.010534748509396903 | Validation loss: 0.009933302683445314\nEpoch 614: Train loss: 0.010592019780435495 | Validation loss: 0.009946095543758323\nEpoch 615: Train loss: 0.010908422529205887 | Validation loss: 0.009975554363336414\nEpoch 616: Train loss: 0.010522779480702916 | Validation loss: 0.009971576063738516\nEpoch 617: Train loss: 0.010552550077490095 | Validation loss: 0.0100377563018507\nEpoch 618: Train loss: 0.010537492958570106 | Validation loss: 0.010151494216794768\nEpoch 619: Train loss: 0.010535564228323184 | Validation loss: 0.00996101948355014\nEpoch 620: Train loss: 0.010596458648150373 | Validation loss: 0.010100075722827265\nEpoch 621: Train loss: 0.010517100357295325 | Validation loss: 0.009971576005530855\nEpoch 622: Train loss: 0.010598121935294734 | Validation loss: 0.010151327141405394\nEpoch 623: Train loss: 0.010538699137719555 | Validation loss: 0.009946962663282951\nEpoch 624: Train loss: 0.011149528879387718 | Validation loss: 0.010012967317985991\nEpoch 625: Train loss: 0.010523004130744893 | Validation loss: 0.010003321920521557\nEpoch 626: Train loss: 0.01056380057384053 | Validation loss: 0.010099550330778584\nEpoch 627: Train loss: 0.01069802612053334 | Validation loss: 0.010115705779753625\nEpoch 628: Train loss: 0.010524551513501339 | Validation loss: 0.009935681378313651\nEpoch 629: Train loss: 0.010512506972493915 | Validation loss: 0.01003306801430881\nEpoch 630: Train loss: 0.010565458580256544 | Validation loss: 0.009973734073961774\nEpoch 631: Train loss: 0.01097021853719424 | Validation loss: 0.009999264361492047\nEpoch 632: Train loss: 0.010532276436505426 | Validation loss: 0.010013339885820946\nEpoch 633: Train loss: 0.010612996893339895 | Validation loss: 0.00993293707142584\nEpoch 634: Train loss: 0.010611463176448725 | Validation loss: 0.009920435113599524\nEpoch 635: Train loss: 0.010510358168782355 | Validation loss: 0.009947321106058856\nEpoch 636: Train loss: 0.010775460302182991 | Validation loss: 0.009966769410918156\nEpoch 637: Train loss: 0.010837817765099721 | Validation loss: 0.009951802727300674\nEpoch 638: Train loss: 0.010671929449826065 | Validation loss: 0.009927506364571551\nEpoch 639: Train loss: 0.010536734315489108 | Validation loss: 0.009997836624582609\nEpoch 640: Train loss: 0.010516416291162992 | Validation loss: 0.01002665437408723\nEpoch 641: Train loss: 0.010564381990762841 | Validation loss: 0.009938398376107216\nEpoch 642: Train loss: 0.010497036537142575 | Validation loss: 0.009908499972273907\nEpoch 643: Train loss: 0.01051052256823621 | Validation loss: 0.010025343023395786\nEpoch 644: Train loss: 0.010543847440405645 | Validation loss: 0.0099373001139611\nEpoch 645: Train loss: 0.010568582718001885 | Validation loss: 0.010121314398323497\nEpoch 646: Train loss: 0.0107201321516186 | Validation loss: 0.00999824246779705\nEpoch 647: Train loss: 0.010537819096094204 | Validation loss: 0.010054888300752888\nEpoch 648: Train loss: 0.010498685309559934 | Validation loss: 0.009979614027542993\nEpoch 649: Train loss: 0.010546520690291395 | Validation loss: 0.0099264623131603\nEpoch 650: Train loss: 0.010540569417773642 | Validation loss: 0.009895757228756944\nEpoch 651: Train loss: 0.010583208974114515 | Validation loss: 0.009891852329019457\nEpoch 652: Train loss: 0.010498929827008396 | Validation loss: 0.009929744691665595\nEpoch 653: Train loss: 0.01056478171570537 | Validation loss: 0.009926742583047599\nEpoch 654: Train loss: 0.010539977390888251 | Validation loss: 0.009930286168431243\nEpoch 655: Train loss: 0.010488347743780145 | Validation loss: 0.010218761356857916\nEpoch 656: Train loss: 0.010701245589492222 | Validation loss: 0.010139957070350647\nEpoch 657: Train loss: 0.010522204535340684 | Validation loss: 0.009905823962374901\nEpoch 658: Train loss: 0.01080498439195152 | Validation loss: 0.01019535077891002\nEpoch 659: Train loss: 0.010565451672947448 | Validation loss: 0.009933067679715654\nEpoch 660: Train loss: 0.010588894976535812 | Validation loss: 0.00993924147527044\nEpoch 661: Train loss: 0.010581655486021191 | Validation loss: 0.009936548284410188\nEpoch 662: Train loss: 0.01052312920607316 | Validation loss: 0.009935579262673855\nEpoch 663: Train loss: 0.010488701926078647 | Validation loss: 0.00990101591257068\nEpoch 664: Train loss: 0.010487991022980876 | Validation loss: 0.00990440690657124\nEpoch 665: Train loss: 0.010482472616584144 | Validation loss: 0.010049296620612344\nEpoch 666: Train loss: 0.010463616558505843 | Validation loss: 0.010038986608075598\nEpoch 667: Train loss: 0.010523895994992927 | Validation loss: 0.009885251725791022\nEpoch 668: Train loss: 0.010499628286601769 | Validation loss: 0.009906966908602044\nEpoch 669: Train loss: 0.01046346957284388 | Validation loss: 0.009880168344049403\nEpoch 670: Train loss: 0.010518260759353224 | Validation loss: 0.009896240449355295\nEpoch 671: Train loss: 0.010773346196704855 | Validation loss: 0.00995103782042861\nEpoch 672: Train loss: 0.010572686270784793 | Validation loss: 0.009914619576496383\nEpoch 673: Train loss: 0.01057859928955117 | Validation loss: 0.009932549689741185\nEpoch 674: Train loss: 0.010971611685818061 | Validation loss: 0.009863063091567406\nEpoch 675: Train loss: 0.010500681036824567 | Validation loss: 0.010115649114595726\nEpoch 676: Train loss: 0.010456278884602297 | Validation loss: 0.009852295906360572\nEpoch 677: Train loss: 0.0104635206840208 | Validation loss: 0.009859705693088472\nEpoch 678: Train loss: 0.01078486545722828 | Validation loss: 0.009932475610791395\nEpoch 679: Train loss: 0.01047251999261789 | Validation loss: 0.009954334799355516\nEpoch 680: Train loss: 0.010465896154831475 | Validation loss: 0.009876679920125753\nEpoch 681: Train loss: 0.010617782242156358 | Validation loss: 0.009873311093542725\nEpoch 682: Train loss: 0.010468084251947908 | Validation loss: 0.009890856851901239\nEpoch 683: Train loss: 0.010439245519389968 | Validation loss: 0.009884527515775213\nEpoch 684: Train loss: 0.010638910083798692 | Validation loss: 0.010053875972516835\nEpoch 685: Train loss: 0.010503750912094902 | Validation loss: 0.009852505386030922\nEpoch 686: Train loss: 0.010472130295561833 | Validation loss: 0.009846382308751345\nEpoch 687: Train loss: 0.010505836828896362 | Validation loss: 0.009828564381071677\nEpoch 688: Train loss: 0.010485940884488324 | Validation loss: 0.009882730451257279\nEpoch 689: Train loss: 0.010476324471205266 | Validation loss: 0.009880127191233138\nEpoch 690: Train loss: 0.010637638986938529 | Validation loss: 0.009882918985870978\nEpoch 691: Train loss: 0.010449155796474466 | Validation loss: 0.009895464987494051\nEpoch 692: Train loss: 0.010554601231382953 | Validation loss: 0.009889930156835666\nEpoch 693: Train loss: 0.010427143463554481 | Validation loss: 0.009908480084656427\nEpoch 694: Train loss: 0.01068203711313092 | Validation loss: 0.009857760645293942\nEpoch 695: Train loss: 0.010523053144829141 | Validation loss: 0.009830974973738194\nEpoch 696: Train loss: 0.012397504030054228 | Validation loss: 0.009985902822033191\nEpoch 697: Train loss: 0.010427878294043088 | Validation loss: 0.009944561849503467\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 698: Train loss: 0.010503241979879223 | Validation loss: 0.009929564703876773\nEpoch 699: Train loss: 0.010529408179637458 | Validation loss: 0.009829248941969126\nEpoch 700: Train loss: 0.010466777929751616 | Validation loss: 0.009829049289692193\nEpoch 701: Train loss: 0.010469900673746856 | Validation loss: 0.009942166099790484\nEpoch 702: Train loss: 0.010458675255196996 | Validation loss: 0.009847428785481801\nEpoch 703: Train loss: 0.010427079270205772 | Validation loss: 0.009836540285808345\nEpoch 704: Train loss: 0.010451393428310338 | Validation loss: 0.009919666044879705\nEpoch 705: Train loss: 0.01045754945759351 | Validation loss: 0.009941020844659457\nEpoch 706: Train loss: 0.010456304133792097 | Validation loss: 0.01009776503390943\nEpoch 707: Train loss: 0.010426614764987284 | Validation loss: 0.010168988063621024\nEpoch 708: Train loss: 0.010551027669053938 | Validation loss: 0.009849120629951358\nEpoch 709: Train loss: 0.010461148443735309 | Validation loss: 0.009913289065783223\nEpoch 710: Train loss: 0.010825139952228509 | Validation loss: 0.009874898610481372\nEpoch 711: Train loss: 0.010477173840627074 | Validation loss: 0.009843566610167423\nEpoch 712: Train loss: 0.01042384735450873 | Validation loss: 0.009889906000656387\nEpoch 713: Train loss: 0.010409708101683969 | Validation loss: 0.010096107396141937\nEpoch 714: Train loss: 0.010626989345635392 | Validation loss: 0.009864054484448085\nEpoch 715: Train loss: 0.010442042752401903 | Validation loss: 0.009828533686231822\nEpoch 716: Train loss: 0.01045907496397073 | Validation loss: 0.009805317919623727\nEpoch 717: Train loss: 0.010505894244286336 | Validation loss: 0.009876228500312815\nEpoch 718: Train loss: 0.010466783556492172 | Validation loss: 0.00985565131607776\nEpoch 719: Train loss: 0.010403230385337438 | Validation loss: 0.009986862695465485\nEpoch 720: Train loss: 0.010420219445626976 | Validation loss: 0.009816664775523046\nEpoch 721: Train loss: 0.010404215279979527 | Validation loss: 0.009815730979122842\nEpoch 722: Train loss: 0.010498327507068299 | Validation loss: 0.009940261292892197\nEpoch 723: Train loss: 0.01045051039222421 | Validation loss: 0.00978479630430229\nEpoch 724: Train loss: 0.010628155041356675 | Validation loss: 0.009838869775800655\nEpoch 725: Train loss: 0.010499249438806955 | Validation loss: 0.009797642773870999\nEpoch 726: Train loss: 0.011517916890119927 | Validation loss: 0.009872885459723571\nEpoch 727: Train loss: 0.010408527453869788 | Validation loss: 0.009914430576221397\nEpoch 728: Train loss: 0.01042487939573928 | Validation loss: 0.009869827006089812\nEpoch 729: Train loss: 0.010410048717555279 | Validation loss: 0.00990893947891891\nEpoch 730: Train loss: 0.010929027300638458 | Validation loss: 0.009809075815913578\nEpoch 731: Train loss: 0.010430705167689465 | Validation loss: 0.009919909721550843\nEpoch 732: Train loss: 0.010393300514099084 | Validation loss: 0.00980573846027255\nEpoch 733: Train loss: 0.011095078739648065 | Validation loss: 0.009785454468025515\nEpoch 734: Train loss: 0.010708121899774091 | Validation loss: 0.00988531686986486\nEpoch 735: Train loss: 0.010484149252685407 | Validation loss: 0.009786209634815654\nEpoch 736: Train loss: 0.010395430840112062 | Validation loss: 0.009797162764395276\nEpoch 737: Train loss: 0.010530606655973114 | Validation loss: 0.009936697800488522\nEpoch 738: Train loss: 0.010422840271429677 | Validation loss: 0.010106207657372579\nEpoch 739: Train loss: 0.010437179204826761 | Validation loss: 0.009791094169486314\nEpoch 740: Train loss: 0.010447810811456293 | Validation loss: 0.009786816022824496\nEpoch 741: Train loss: 0.01038774667863941 | Validation loss: 0.009766761436670398\nEpoch 742: Train loss: 0.010426004740616513 | Validation loss: 0.009776733175385743\nEpoch 743: Train loss: 0.010448177345097065 | Validation loss: 0.009788679589595025\nEpoch 744: Train loss: 0.010422508258165585 | Validation loss: 0.010096880490891635\nEpoch 745: Train loss: 0.010403994399692036 | Validation loss: 0.009763326563794786\nEpoch 746: Train loss: 0.010366135932902884 | Validation loss: 0.009776671174525594\nEpoch 747: Train loss: 0.010523456847295165 | Validation loss: 0.009975501268248385\nEpoch 748: Train loss: 0.010406101051355816 | Validation loss: 0.009763270296389237\nEpoch 749: Train loss: 0.010440340230060328 | Validation loss: 0.009816890019768229\nEpoch 750: Train loss: 0.012847373785916716 | Validation loss: 0.009760182389679054\nEpoch 751: Train loss: 0.010646137463886084 | Validation loss: 0.009781195782124996\nEpoch 752: Train loss: 0.010581852793822892 | Validation loss: 0.009828514759040749\nEpoch 753: Train loss: 0.010398481079998115 | Validation loss: 0.0097857933336248\nEpoch 754: Train loss: 0.010412888287747692 | Validation loss: 0.009875551448203623\nEpoch 755: Train loss: 0.010405061669492474 | Validation loss: 0.009815045166760683\nEpoch 756: Train loss: 0.010394782946807228 | Validation loss: 0.009755795579015588\nEpoch 757: Train loss: 0.010397570117169784 | Validation loss: 0.009909163617218534\nEpoch 758: Train loss: 0.010422406763407506 | Validation loss: 0.009864578198175877\nEpoch 759: Train loss: 0.010436007717443217 | Validation loss: 0.009782253192194426\nEpoch 760: Train loss: 0.010367880171139 | Validation loss: 0.009748341117907936\nEpoch 761: Train loss: 0.010409792063809518 | Validation loss: 0.009759667426502952\nEpoch 762: Train loss: 0.010496855538804084 | Validation loss: 0.009832387052786848\nEpoch 763: Train loss: 0.010387778023464812 | Validation loss: 0.009833018411882222\nEpoch 764: Train loss: 0.010407965022346212 | Validation loss: 0.009779171358483532\nEpoch 765: Train loss: 0.010568920461486818 | Validation loss: 0.00992169564900299\nEpoch 766: Train loss: 0.01047360314664224 | Validation loss: 0.009882506293555101\nEpoch 767: Train loss: 0.010358683480363753 | Validation loss: 0.009842460897440711\nEpoch 768: Train loss: 0.01036309807548403 | Validation loss: 0.009796513031081608\nEpoch 769: Train loss: 0.01081701748060166 | Validation loss: 0.009829464155094078\nEpoch 770: Train loss: 0.010460672026965767 | Validation loss: 0.009794255330537757\nEpoch 771: Train loss: 0.010361758002545685 | Validation loss: 0.009815518753991151\nEpoch 772: Train loss: 0.010346114046923403 | Validation loss: 0.009757886582519859\nEpoch 773: Train loss: 0.01041540790659686 | Validation loss: 0.009884599247016013\nEpoch 774: Train loss: 0.01034502754919231 | Validation loss: 0.00977798529008093\nEpoch 775: Train loss: 0.010530738082403937 | Validation loss: 0.00975583765345315\nEpoch 776: Train loss: 0.010378506636091819 | Validation loss: 0.009877384756691754\nEpoch 777: Train loss: 0.010477243951754645 | Validation loss: 0.009838989935815334\nEpoch 778: Train loss: 0.010403797965005247 | Validation loss: 0.00973457876049603\nEpoch 779: Train loss: 0.010485345659415342 | Validation loss: 0.009760915379350385\nEpoch 780: Train loss: 0.010583517717249278 | Validation loss: 0.0097479719746237\nEpoch 781: Train loss: 0.010362925273107572 | Validation loss: 0.009761022597861787\nEpoch 782: Train loss: 0.01052782231191587 | Validation loss: 0.009713784471387044\nEpoch 783: Train loss: 0.010455992515845638 | Validation loss: 0.009801590097292015\nEpoch 784: Train loss: 0.010407872963696718 | Validation loss: 0.009753303951583803\nEpoch 785: Train loss: 0.010638351823824147 | Validation loss: 0.0098154398050004\nEpoch 786: Train loss: 0.010442675058988648 | Validation loss: 0.009778701797282944\nEpoch 787: Train loss: 0.010485115176480677 | Validation loss: 0.009859223869473984\nEpoch 788: Train loss: 0.01034941980873959 | Validation loss: 0.009743907876933614\nEpoch 789: Train loss: 0.010348796582547948 | Validation loss: 0.009774269458527366\nEpoch 790: Train loss: 0.01047815255070519 | Validation loss: 0.009761559427715838\nEpoch 791: Train loss: 0.010451028107329167 | Validation loss: 0.009791315824259073\nEpoch 792: Train loss: 0.010384710929227166 | Validation loss: 0.009742660195721934\nEpoch 793: Train loss: 0.010562367756695798 | Validation loss: 0.009713588340673596\nEpoch 794: Train loss: 0.010384790980929716 | Validation loss: 0.009945408479931453\nEpoch 795: Train loss: 0.010378324769489054 | Validation loss: 0.009700773167423904\nEpoch 796: Train loss: 0.010332158627635282 | Validation loss: 0.009773544773148993\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 797: Train loss: 0.010422348698031984 | Validation loss: 0.009718070340265209\nEpoch 798: Train loss: 0.010381234602795707 | Validation loss: 0.009990701869052524\nEpoch 799: Train loss: 0.010381650551506836 | Validation loss: 0.009790731011889875\nEpoch 800: Train loss: 0.010320318431089012 | Validation loss: 0.009792836441192776\nEpoch 801: Train loss: 0.01038067296354307 | Validation loss: 0.009786122672570249\nEpoch 802: Train loss: 0.010472437961854868 | Validation loss: 0.009753886901307851\nEpoch 803: Train loss: 0.010395544380622191 | Validation loss: 0.009716794680571184\nEpoch 804: Train loss: 0.010365844290289614 | Validation loss: 0.009737365772404397\nEpoch 805: Train loss: 0.010410352383688506 | Validation loss: 0.009707458521006629\nEpoch 806: Train loss: 0.010365238254760496 | Validation loss: 0.009786519435389588\nEpoch 807: Train loss: 0.010361824453058135 | Validation loss: 0.00978001362333695\nEpoch 808: Train loss: 0.01032776079308759 | Validation loss: 0.009691265712414557\nEpoch 809: Train loss: 0.010394580723691938 | Validation loss: 0.00983662724805375\nEpoch 810: Train loss: 0.010351935867220163 | Validation loss: 0.009745309148759892\nEpoch 811: Train loss: 0.010332222190400999 | Validation loss: 0.009681586884350205\nEpoch 812: Train loss: 0.010318760181285648 | Validation loss: 0.009717785277947163\nEpoch 813: Train loss: 0.010329242035772445 | Validation loss: 0.009730110915067295\nEpoch 814: Train loss: 0.01072870871737703 | Validation loss: 0.00974031500906373\nEpoch 815: Train loss: 0.010329419048503041 | Validation loss: 0.00977348976690943\nEpoch 816: Train loss: 0.010709851685937287 | Validation loss: 0.009723657780947784\nEpoch 817: Train loss: 0.010534955576683084 | Validation loss: 0.009693834241867686\nEpoch 818: Train loss: 0.010334702017846413 | Validation loss: 0.010012278469124189\nEpoch 819: Train loss: 0.010417592482149808 | Validation loss: 0.009672034337806204\nEpoch 820: Train loss: 0.010365414471986393 | Validation loss: 0.009758676324660579\nEpoch 821: Train loss: 0.010439907239439586 | Validation loss: 0.009747560737499347\nEpoch 822: Train loss: 0.010325741110136732 | Validation loss: 0.009689769435984394\nEpoch 823: Train loss: 0.01031582745943322 | Validation loss: 0.00979465400450863\nEpoch 824: Train loss: 0.010320048717807772 | Validation loss: 0.00969573893235065\nEpoch 825: Train loss: 0.010316965781385079 | Validation loss: 0.00995052510794873\nEpoch 826: Train loss: 0.010355673455503873 | Validation loss: 0.009693537664134055\nEpoch 827: Train loss: 0.010363444931701653 | Validation loss: 0.009691745935318371\nEpoch 828: Train loss: 0.010327340097218338 | Validation loss: 0.009683785581728444\nEpoch 829: Train loss: 0.010322101472411305 | Validation loss: 0.009665782233544936\nEpoch 830: Train loss: 0.010289646031904139 | Validation loss: 0.009783373990406593\nEpoch 831: Train loss: 0.010448910829533512 | Validation loss: 0.009707225215000411\nEpoch 832: Train loss: 0.010323520756274875 | Validation loss: 0.009688463702332228\nEpoch 833: Train loss: 0.010412707779323682 | Validation loss: 0.009692966181319207\nEpoch 834: Train loss: 0.0103003635433399 | Validation loss: 0.009807223153378194\nEpoch 835: Train loss: 0.010300353774154145 | Validation loss: 0.009711210324894637\nEpoch 836: Train loss: 0.010295823934332779 | Validation loss: 0.009792328074884912\nEpoch 837: Train loss: 0.010299629557670819 | Validation loss: 0.009835927300931266\nEpoch 838: Train loss: 0.010324776670636816 | Validation loss: 0.00975815553101711\nEpoch 839: Train loss: 0.010391427077896273 | Validation loss: 0.009720744526324173\nEpoch 840: Train loss: 0.010348196516537832 | Validation loss: 0.009755968475171054\nEpoch 841: Train loss: 0.0102690572020947 | Validation loss: 0.009661742292034129\nEpoch 842: Train loss: 0.010453485867603578 | Validation loss: 0.009731341095175594\nEpoch 843: Train loss: 0.01032369319970409 | Validation loss: 0.009648404084146023\nEpoch 844: Train loss: 0.010289076993811049 | Validation loss: 0.009676273659958193\nEpoch 845: Train loss: 0.010553798224363063 | Validation loss: 0.009686967784849307\nEpoch 846: Train loss: 0.010264509737478673 | Validation loss: 0.009857817698502913\nEpoch 847: Train loss: 0.010276944740326144 | Validation loss: 0.009782764391275123\nEpoch 848: Train loss: 0.010292521241353825 | Validation loss: 0.00977190281264484\nEpoch 849: Train loss: 0.010298153364500549 | Validation loss: 0.010016055583643416\nEpoch 850: Train loss: 0.010311101446859539 | Validation loss: 0.009638803370762616\nEpoch 851: Train loss: 0.010434059377682084 | Validation loss: 0.0096567632475247\nEpoch 852: Train loss: 0.0103021674860631 | Validation loss: 0.009769504812235633\nEpoch 853: Train loss: 0.01027974254813873 | Validation loss: 0.009781874774489552\nEpoch 854: Train loss: 0.01034646040514215 | Validation loss: 0.009923892891189704\nEpoch 855: Train loss: 0.010868639442681646 | Validation loss: 0.009682491732140383\nEpoch 856: Train loss: 0.01030578116963928 | Validation loss: 0.009729024682504436\nEpoch 857: Train loss: 0.010266229416528303 | Validation loss: 0.009798321465495974\nEpoch 858: Train loss: 0.010293923143763095 | Validation loss: 0.00966480968054384\nEpoch 859: Train loss: 0.010258143368345272 | Validation loss: 0.009692305194524428\nEpoch 860: Train loss: 0.010268826328683645 | Validation loss: 0.009956721934334686\nEpoch 861: Train loss: 0.01046077579505638 | Validation loss: 0.00963282246569482\nEpoch 862: Train loss: 0.010284592371640934 | Validation loss: 0.009718294837512076\nEpoch 863: Train loss: 0.010434847978305899 | Validation loss: 0.009756135967715332\nEpoch 864: Train loss: 0.010253429742685208 | Validation loss: 0.009679516563968113\nEpoch 865: Train loss: 0.01028940674667764 | Validation loss: 0.009696631925180554\nEpoch 866: Train loss: 0.010724710937615277 | Validation loss: 0.009628698957385495\nEpoch 867: Train loss: 0.010686507590839433 | Validation loss: 0.00969805479204903\nEpoch 868: Train loss: 0.01027602942000764 | Validation loss: 0.009665733106279125\nEpoch 869: Train loss: 0.010240465565907976 | Validation loss: 0.009626721187184254\nEpoch 870: Train loss: 0.010251324143609963 | Validation loss: 0.00971093337284401\nEpoch 871: Train loss: 0.010263513537615331 | Validation loss: 0.009632935475868484\nEpoch 872: Train loss: 0.010280622460413724 | Validation loss: 0.009664329768080885\nEpoch 873: Train loss: 0.01069088332911229 | Validation loss: 0.009727816970553249\nEpoch 874: Train loss: 0.010357045387435291 | Validation loss: 0.009690246341051534\nEpoch 875: Train loss: 0.010255809740758397 | Validation loss: 0.009691027206523964\nEpoch 876: Train loss: 0.010387760153712912 | Validation loss: 0.009711518194914484\nEpoch 877: Train loss: 0.010732822572915919 | Validation loss: 0.009686339258526763\nEpoch 878: Train loss: 0.01027116550055022 | Validation loss: 0.00974595306130747\nEpoch 879: Train loss: 0.010318768660201587 | Validation loss: 0.009845929821797958\nEpoch 880: Train loss: 0.01033706088977245 | Validation loss: 0.009626832092180848\nEpoch 881: Train loss: 0.010265342745697126 | Validation loss: 0.009678163468682518\nEpoch 882: Train loss: 0.010232832363625575 | Validation loss: 0.009642127843108028\nEpoch 883: Train loss: 0.010569450717108944 | Validation loss: 0.009690722275990993\nEpoch 884: Train loss: 0.010248399265239842 | Validation loss: 0.009629144871723838\nEpoch 885: Train loss: 0.0103657241173399 | Validation loss: 0.009652412380091846\nEpoch 886: Train loss: 0.010232402912353993 | Validation loss: 0.00965711468597874\nEpoch 887: Train loss: 0.0102419709340514 | Validation loss: 0.00969185383291915\nEpoch 888: Train loss: 0.01027777685602713 | Validation loss: 0.009954462739794204\nEpoch 889: Train loss: 0.01022677008424782 | Validation loss: 0.009843883434465775\nEpoch 890: Train loss: 0.010272847022861242 | Validation loss: 0.009721145286069563\nEpoch 891: Train loss: 0.010451680172183033 | Validation loss: 0.009746051936720809\nEpoch 892: Train loss: 0.010240932415601693 | Validation loss: 0.009669131172510484\nEpoch 893: Train loss: 0.010264771515115475 | Validation loss: 0.009623993071727455\nEpoch 894: Train loss: 0.010261166472143183 | Validation loss: 0.0096263158096311\nEpoch 895: Train loss: 0.010241534499477388 | Validation loss: 0.009733673544057334\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 896: Train loss: 0.010235579676292319 | Validation loss: 0.009641818255962184\nEpoch 897: Train loss: 0.010230623489607953 | Validation loss: 0.009617473251031091\nEpoch 898: Train loss: 0.010411088345184302 | Validation loss: 0.009742413007188588\nEpoch 899: Train loss: 0.010226366407651868 | Validation loss: 0.009945057720566789\nEpoch 900: Train loss: 0.010215860937023535 | Validation loss: 0.009617733652703464\nEpoch 901: Train loss: 0.010255478710557023 | Validation loss: 0.009655138341865191\nEpoch 902: Train loss: 0.010216783832422353 | Validation loss: 0.00960400050583606\nEpoch 903: Train loss: 0.010242134238877851 | Validation loss: 0.009698089823359624\nEpoch 904: Train loss: 0.0102841641217108 | Validation loss: 0.00963337040351083\nEpoch 905: Train loss: 0.010202347834516937 | Validation loss: 0.009575720663027218\nEpoch 906: Train loss: 0.01021870181365456 | Validation loss: 0.009627893955136338\nEpoch 907: Train loss: 0.010227553656376485 | Validation loss: 0.00966139950711901\nEpoch 908: Train loss: 0.01038336920707176 | Validation loss: 0.009684135962743312\nEpoch 909: Train loss: 0.0102881326553567 | Validation loss: 0.009701228399838632\nEpoch 910: Train loss: 0.010226577261669768 | Validation loss: 0.009619472897611558\nEpoch 911: Train loss: 0.010281371272867545 | Validation loss: 0.009763885784195736\nEpoch 912: Train loss: 0.010230369506947076 | Validation loss: 0.00963963174338763\nEpoch 913: Train loss: 0.010342460025438212 | Validation loss: 0.009608887097177407\nEpoch 914: Train loss: 0.01029224309604615 | Validation loss: 0.009635561543594426\nEpoch 915: Train loss: 0.010234174706662694 | Validation loss: 0.009598939640757939\nEpoch 916: Train loss: 0.010251320248547321 | Validation loss: 0.009811672595484803\nEpoch 917: Train loss: 0.010211239193773104 | Validation loss: 0.009622973661559323\nEpoch 918: Train loss: 0.010193318713896183 | Validation loss: 0.00961976441127869\nEpoch 919: Train loss: 0.010281116290975155 | Validation loss: 0.00975271845042395\nEpoch 920: Train loss: 0.010193117360662049 | Validation loss: 0.009626320000582686\nEpoch 921: Train loss: 0.01020930538005713 | Validation loss: 0.009580981529628238\nEpoch 922: Train loss: 0.010199578716613663 | Validation loss: 0.009619348449632525\nEpoch 923: Train loss: 0.01026025709709049 | Validation loss: 0.0096626427063408\nEpoch 924: Train loss: 0.010191048744470917 | Validation loss: 0.009628189271703983\nEpoch 925: Train loss: 0.01033163195501806 | Validation loss: 0.009859687357675284\nEpoch 926: Train loss: 0.010244594178705787 | Validation loss: 0.009641509590437636\nEpoch 927: Train loss: 0.010448317657897456 | Validation loss: 0.00956296173778052\nEpoch 928: Train loss: 0.010202594256649414 | Validation loss: 0.00968808044369022\nEpoch 929: Train loss: 0.010659964376827702 | Validation loss: 0.009670203153897697\nEpoch 930: Train loss: 0.010257332165363349 | Validation loss: 0.009831115312408656\nEpoch 931: Train loss: 0.010203137541086309 | Validation loss: 0.009628169549008211\nEpoch 932: Train loss: 0.01026484392867941 | Validation loss: 0.009602166004090881\nEpoch 933: Train loss: 0.010226917097396735 | Validation loss: 0.009565867076162249\nEpoch 934: Train loss: 0.010234206779083857 | Validation loss: 0.009920454380335286\nEpoch 935: Train loss: 0.010204818174113624 | Validation loss: 0.009628521132981405\nEpoch 936: Train loss: 0.010186323976894427 | Validation loss: 0.009602753348493328\nEpoch 937: Train loss: 0.01020150035361035 | Validation loss: 0.009608835816228142\nEpoch 938: Train loss: 0.01022130900592957 | Validation loss: 0.009609609910209352\nEpoch 939: Train loss: 0.010246870234065378 | Validation loss: 0.0096427645185031\nEpoch 940: Train loss: 0.010166747365373239 | Validation loss: 0.00984567940273943\nEpoch 941: Train loss: 0.010253316257148981 | Validation loss: 0.0096880996619196\nEpoch 942: Train loss: 0.010229862385636402 | Validation loss: 0.00962346131564118\nEpoch 943: Train loss: 0.010211026108461536 | Validation loss: 0.00961667756200768\nEpoch 944: Train loss: 0.01024894254320922 | Validation loss: 0.009593624738045037\nEpoch 945: Train loss: 0.010251560817576118 | Validation loss: 0.009570726135279983\nEpoch 946: Train loss: 0.01018300592356051 | Validation loss: 0.009573249088134617\nEpoch 947: Train loss: 0.010215804378579682 | Validation loss: 0.009591083692309136\nEpoch 948: Train loss: 0.010210729831467487 | Validation loss: 0.009575956753299883\nEpoch 949: Train loss: 0.010503620594843395 | Validation loss: 0.009570518770487979\nEpoch 950: Train loss: 0.010206463125844797 | Validation loss: 0.009630337085885307\nEpoch 951: Train loss: 0.010176427545957267 | Validation loss: 0.009635372339592626\nEpoch 952: Train loss: 0.010165411396883428 | Validation loss: 0.00957672424071158\nEpoch 953: Train loss: 0.010478977379130406 | Validation loss: 0.009547131912161907\nEpoch 954: Train loss: 0.010186205925290577 | Validation loss: 0.009543576054663086\nEpoch 955: Train loss: 0.010172062802464805 | Validation loss: 0.009550026210490614\nEpoch 956: Train loss: 0.010365391357077492 | Validation loss: 0.009569899974546084\nEpoch 957: Train loss: 0.010266604218890684 | Validation loss: 0.009574246088353297\nEpoch 958: Train loss: 0.010173149494221434 | Validation loss: 0.009554762218613178\nEpoch 959: Train loss: 0.010193867767359026 | Validation loss: 0.009626176082141077\nEpoch 960: Train loss: 0.010388170815228174 | Validation loss: 0.009628806913193936\nEpoch 961: Train loss: 0.010277553636115044 | Validation loss: 0.00962428313990434\nEpoch 962: Train loss: 0.010160112420433305 | Validation loss: 0.009539511811453849\nEpoch 963: Train loss: 0.01023256406834763 | Validation loss: 0.0096385714035326\nEpoch 964: Train loss: 0.010152818780625239 | Validation loss: 0.009563521802192554\nEpoch 965: Train loss: 0.010225744096614007 | Validation loss: 0.009614078376519805\nEpoch 966: Train loss: 0.010175850889128115 | Validation loss: 0.009594046316730479\nEpoch 967: Train loss: 0.010381776655170446 | Validation loss: 0.009572871242805073\nEpoch 968: Train loss: 0.010305950376075797 | Validation loss: 0.00956068680776904\nEpoch 969: Train loss: 0.01021165146246656 | Validation loss: 0.009710352091739574\nEpoch 970: Train loss: 0.010129562638313574 | Validation loss: 0.009589189760542164\nEpoch 971: Train loss: 0.012589153966271423 | Validation loss: 0.009550760548639422\nEpoch 972: Train loss: 0.010148686408582661 | Validation loss: 0.009518998422815153\nEpoch 973: Train loss: 0.01087431853132633 | Validation loss: 0.009580508301345011\nEpoch 974: Train loss: 0.010148750619717047 | Validation loss: 0.009520574938505888\nEpoch 975: Train loss: 0.010242781178223796 | Validation loss: 0.009528053148339191\nEpoch 976: Train loss: 0.010142186286328878 | Validation loss: 0.009523853281280026\nEpoch 977: Train loss: 0.010377626581531432 | Validation loss: 0.009611801690577218\nEpoch 978: Train loss: 0.01014092581883435 | Validation loss: 0.009561651793774217\nEpoch 979: Train loss: 0.010149790779299414 | Validation loss: 0.009525595814920962\nEpoch 980: Train loss: 0.010945596709562879 | Validation loss: 0.009675359178800136\nEpoch 981: Train loss: 0.010187868142060729 | Validation loss: 0.00956492058079069\nEpoch 982: Train loss: 0.010209238796960562 | Validation loss: 0.009497722116066143\nEpoch 983: Train loss: 0.010146653641843133 | Validation loss: 0.009559764410369098\nEpoch 984: Train loss: 0.010120154471628161 | Validation loss: 0.009610213009485355\nEpoch 985: Train loss: 0.010454005183419213 | Validation loss: 0.009572973377847424\nEpoch 986: Train loss: 0.010276821141208833 | Validation loss: 0.009559078859941414\nEpoch 987: Train loss: 0.010125213237996731 | Validation loss: 0.00955102340473483\nEpoch 988: Train loss: 0.010142632119823247 | Validation loss: 0.00955992283221955\nEpoch 989: Train loss: 0.010139232135972835 | Validation loss: 0.009514001721981913\nEpoch 990: Train loss: 0.010136945502987752 | Validation loss: 0.009494672461490458\nEpoch 991: Train loss: 0.010145021279135512 | Validation loss: 0.009505144679375613\nEpoch 992: Train loss: 0.010187038821944347 | Validation loss: 0.00950697879306972\nEpoch 993: Train loss: 0.01018207875959989 | Validation loss: 0.009497090650256723\nEpoch 994: Train loss: 0.010179216475484686 | Validation loss: 0.009557494272788366\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 995: Train loss: 0.010142405173003985 | Validation loss: 0.009478694526478648\nEpoch 996: Train loss: 0.010138451464525942 | Validation loss: 0.009617242690486213\nEpoch 997: Train loss: 0.010174522215190033 | Validation loss: 0.009494140608391414\nEpoch 998: Train loss: 0.01010907660060588 | Validation loss: 0.009719558292999864\nEpoch 999: Train loss: 0.010303899571428903 | Validation loss: 0.009496528936627632\nEpoch 1000: Train loss: 0.010173246439080685 | Validation loss: 0.009599401072288552\nFinished Training\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f89f4d3707e823977cc5290aa19db3792c667ab3"
      },
      "cell_type": "code",
      "source": "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=True, num_workers=0)\nevaluate(test_model, test_loader, torch.nn.MSELoss())",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "0.010839870306275165"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}